{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Backgammon as B\n",
    "import agent as A\n",
    "import flipped_agent as FA\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backgammon:\n",
    "    def __init__(self):\n",
    "        self.board = B.init_board()\n",
    "            \n",
    "    def reset(self):\n",
    "        self.board = B.init_board()\n",
    "        self.done = False\n",
    "    \n",
    "    def legal_moves(self, dice, player):\n",
    "        moves, boards = B.legal_moves(board = self.board, dice = dice, player = player)\n",
    "        if len(boards) == 0:\n",
    "            return [], []\n",
    "        boards = np.vstack(boards)\n",
    "        return moves, boards\n",
    "    \n",
    "    def swap_player(self):\n",
    "        self.board = FA.flip_board(board_copy=np.copy(self.board))\n",
    "    \n",
    "    # oppents random move\n",
    "    def make_move(self, dice):\n",
    "        moves, _ = self.legal_moves(dice, -1)\n",
    "        if len(moves) == 0:\n",
    "            return self.step([], -1)\n",
    "        move = moves[np.random.randint(len(moves))]\n",
    "        return self.step(move, -1)\n",
    "    \n",
    "    def step(self, move, player):\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                self.board = B.update_board(board = self.board, move = m, player = player)\n",
    "        reward = 0\n",
    "        self.done = False\n",
    "        if self.iswin():\n",
    "            reward = player\n",
    "            self.done = True\n",
    "        return np.copy(self.board), reward, self.done\n",
    "    \n",
    "    def symbolic_step(self, move):\n",
    "        board = np.copy(self.board)\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                board = B.update_board(board = board, move = m, player = 1)\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if B.game_over(board):\n",
    "            reward = 1\n",
    "            done = True\n",
    "        return board, reward, self.done\n",
    "        \n",
    "    def iswin(self):\n",
    "        return B.game_over(self.board)\n",
    "        \n",
    "    def render(self):\n",
    "        B.pretty_print(self.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolicyGrad Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, sess, gamma = 0.99, learning_rate = 1e-3, entropy = 0.1,\n",
    "                epsilon = 1, epsdecay = 0.999):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        self._epsilon = epsilon\n",
    "        self._epsdecay = epsdecay\n",
    "        \n",
    "        self._states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"afterstates\")\n",
    "        self._done = tf.placeholder(\"float32\", (None, ), name = \"dones\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, ), name = \"rewards\")\n",
    "        \n",
    "        # Actor\n",
    "        self.network = keras.models.Sequential()\n",
    "        self.network.add(L.Dense(32))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Dense(32))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Dense(1))\n",
    "        \n",
    "        # Predictions\n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self.network(self._states)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_entropy = -tf.reduce_sum(self._actor_policy * self._actor_log_policy)\n",
    "        \n",
    "        # Losses\n",
    "        self._actor_loss = -tf.reduce_sum(self._actor_log_policy * self._cumulative_rewards)\n",
    "        self._actor_loss -= entropy * self._actor_entropy\n",
    "        \n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self._update = self._optimizer.minimize(self._actor_loss)\n",
    "        \n",
    "        self._s = sess\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._states: states})).flatten()\n",
    "        if np.random.uniform() < self._epsilon:\n",
    "            action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "        else:\n",
    "            action = np.argmax(probs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def update(self, states, rewards, afterstates, done):\n",
    "        self._s.run(self._update, \n",
    "                    ({self._states: states, \n",
    "                      self._cumulative_rewards: rewards}))\n",
    "        self._epsilon *= self._epsdecay\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "        r = 0.\n",
    "        for i, reward in enumerate(reversed(rewards)):\n",
    "            r += reward\n",
    "            R[-(i + 1)] = r\n",
    "            r *= self._gamma\n",
    "        return R\n",
    "    \n",
    "    def ExamplePolicy(self):\n",
    "        _, st = B.legal_moves(B.init_board(), B.roll_dice(), 1)\n",
    "        \n",
    "        out = np.round(self._s.run(self._actor_policy, ({self._states: st})) * 100)/100\n",
    "        out = out.flatten()\n",
    "        out.sort()\n",
    "        return out[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic (Shared network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, sess, gamma = 0.99, learning_rate = 1e-3, entropy = 0.1,\n",
    "                epsilon = 1, epsdecay = 0.999):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        self._epsilon = epsilon\n",
    "        self._epsdecay = epsdecay\n",
    "        \n",
    "        self._states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "        self._currstates = tf.placeholder(\"float32\", (None, 29), name = \"currstates\")\n",
    "        #self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"afterstates\")\n",
    "        self._done = tf.placeholder(\"float32\", (None, ), name = \"dones\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, ), name = \"rewards\")\n",
    "        \n",
    "        # Actor\n",
    "        self.network = keras.models.Sequential()\n",
    "        self.network.add(L.Dense(32))\n",
    "        self.network.add(L.BatchNormalization())\n",
    "        self.network.add(L.PReLU())\n",
    "        self.network.add(L.Dense(64))\n",
    "        self.network.add(L.BatchNormalization())\n",
    "        self.network.add(L.PReLU())\n",
    "        self.network.add(L.Dense(32))\n",
    "        self.network.add(L.BatchNormalization())\n",
    "        self.network.add(L.PReLU())\n",
    "        self.network.add(L.Dense(1))\n",
    "        \n",
    "        # Predictions\n",
    "        ## Critic\n",
    "        self._state_values = tf.nn.tanh(self.network(self._currstates))\n",
    "        self._afterstate_values = tf.nn.tanh(self.network(self._states)) * (1 - self._done)\n",
    "        \n",
    "        self._target_state_values = self._cumulative_rewards\n",
    "        self._target_state_values += self._gamma * self._afterstate_values * (1 - self._done)\n",
    "        \n",
    "        self._advantage = self._target_state_values - self._state_values\n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self.network(self._states)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_entropy = -tf.reduce_mean(self._actor_policy * self._actor_log_policy)\n",
    "        \n",
    "        # Losses\n",
    "        self._critic_loss = -tf.reduce_mean(tf.stop_gradient(self._advantage) * self._state_values)\n",
    "        self._actor_loss = -tf.reduce_mean(self._actor_log_policy * tf.stop_gradient(self._advantage))\n",
    "        self._actor_loss -= entropy * self._actor_entropy\n",
    "        \n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self._update = self._optimizer.minimize(self._actor_loss + self._critic_loss)\n",
    "        \n",
    "        self._s = sess\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._states: states})).flatten()\n",
    "        if np.random.uniform() < self._epsilon:\n",
    "            action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "        else:\n",
    "            action = np.argmax(probs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def update(self, states, currstates, rewards, done):\n",
    "        self._s.run(self._update, \n",
    "                    ({self._states: states, \n",
    "                      self._currstates: currstates,\n",
    "                      self._done: done,\n",
    "                      self._cumulative_rewards: rewards}))\n",
    "        self._epsilon *= self._epsdecay\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "        r = 0.\n",
    "        for i, reward in enumerate(reversed(rewards)):\n",
    "            r += reward\n",
    "            R[-(i + 1)] = r\n",
    "            r *= self._gamma\n",
    "        return R\n",
    "    \n",
    "    def ExamplePolicy(self):\n",
    "        _, st = B.legal_moves(B.init_board(), B.roll_dice(), 1)\n",
    "        \n",
    "        out = np.round(self._s.run(self._actor_policy, ({self._states: st})) * 100)/100\n",
    "        out = out.flatten()\n",
    "        out.sort()\n",
    "        return out[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-5-02a9ee519b92>, line 70)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-02a9ee519b92>\"\u001b[0;36m, line \u001b[0;32m70\u001b[0m\n\u001b[0;31m    if type != \"compete\"\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def PlayGame(player1, player2 = \"random\"):\n",
    "    \n",
    "    env = backgammon()\n",
    "    \n",
    "    players = [player1, player2]\n",
    "    \n",
    "    active = np.random.randint(2)\n",
    "    \n",
    "    currstates = [[], []]\n",
    "    states = [[], []]\n",
    "    rewards = [[], []]\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        dice = B.roll_dice()\n",
    "        for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "            possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "            n_actions = len(possible_moves)\n",
    "\n",
    "            if n_actions == 0:\n",
    "                break\n",
    "\n",
    "            currstates[active].append(env.board)\n",
    "            action = players[active].sample_action(possible_boards)\n",
    "            new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "            rewards[active].append(reward)\n",
    "            states[active].append(new_board)\n",
    "            afterstates[active].append(old_board)\n",
    "\n",
    "            if done:\n",
    "                break\n",
    "            \n",
    "            env.swap_player()\n",
    "            active = (active + 1) % 2\n",
    "\n",
    "        if not done:\n",
    "            dice = B.roll_dice()\n",
    "            \n",
    "            if player2 != \"random\":\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "\n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "\n",
    "                        action = player2.sample_action(possible_boards)\n",
    "\n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                env.swap_player()\n",
    "                \n",
    "            else:\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                    old_state, reward, done = env.make_move(dice)\n",
    "                    if done:\n",
    "                        rewards[-1] = -1\n",
    "                        break\n",
    "\n",
    "    afterstates.append(old_board)\n",
    "    afterstates = afterstates[2:]\n",
    "\n",
    "    Dones = np.zeros(len(states))\n",
    "    Dones[-1] = 1\n",
    "\n",
    "    States = np.vstack(states)\n",
    "    CumulativeRewards = player.get_cumulative_rewards(rewards)\n",
    "    AfterStates = np.vstack(afterstates)\n",
    "\n",
    "    if type != \"compete\"\n",
    "        player1.update(states = States, \n",
    "                      rewards = CumulativeRewards,\n",
    "                      afterstates = AfterStates, \n",
    "                      done = Dones)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/bgautijonsson/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py:1662: UserWarning: An interactive session is already active. This can cause out-of-memory errors in some cases. You must explicitly call `InteractiveSession.close()` to release resources held by the other session(s).\n",
      "  warnings.warn('An interactive session is already active. This can '\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = ActorCritic(sess = s, entropy = 0.1, learning_rate = 0.00025, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "PG = PolicyGradient(sess = s, entropy = 0.0, learning_rate=1e-4, gamma = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_44 (Dense)             (None, 32)                960       \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "p_re_lu_4 (PReLU)            (None, 32)                32        \n",
      "_________________________________________________________________\n",
      "dense_45 (Dense)             (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "p_re_lu_5 (PReLU)            (None, 64)                64        \n",
      "_________________________________________________________________\n",
      "dense_46 (Dense)             (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 32)                128       \n",
      "_________________________________________________________________\n",
      "p_re_lu_6 (PReLU)            (None, 32)                32        \n",
      "_________________________________________________________________\n",
      "dense_47 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,825\n",
      "Trainable params: 5,569\n",
      "Non-trainable params: 256\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "AC.network.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.06, 0.06, 0.05, 0.05, 0.05, 0.04, 0.04, 0.04, 0.04, 0.04, 0.04,\n",
       "       0.04, 0.04, 0.04, 0.04, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03, 0.03,\n",
       "       0.03, 0.02, 0.02, 0.02, 0.02], dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC.ExamplePolicy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stakt episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spila við sjálft sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage:  0.49\n",
      "Agent epsilon:  0.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example policy: \n",
      " [0.25 0.25 0.11 0.11 0.06 0.06 0.04 0.04 0.01 0.01 0.01 0.01 0.01 0.01\n",
      " 0.01 0.01 0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.   0.\n",
      " 0.   0.   0.  ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-0e61eb28e341>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     29\u001b[0m                 \u001b[0mcurrstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m                 \u001b[0;31m#afterstates.append(env.board)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_boards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m                 \u001b[0mnew_board\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-68-b5ff792fb3f8>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_epsilon\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        states = []\n",
    "        currstates = []\n",
    "        #afterstates = []\n",
    "        rewards = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                currstates.append(env.board)\n",
    "                #afterstates.append(env.board)\n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                states.append(new_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = AC.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                            \n",
    "        #afterstates.append(new_board)\n",
    "        #afterstates = afterstates[2:]\n",
    "        \n",
    "        Dones = np.zeros(len(states))\n",
    "        Dones[-1] = 1\n",
    "        \n",
    "        States = np.vstack(states)\n",
    "        CumulativeRewards = AC.get_cumulative_rewards(rewards)\n",
    "        CurrStates = np.vstack(currstates)\n",
    "        #AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        AC.update(states = States, \n",
    "                  currstates = CurrStates,\n",
    "                  #afterstates = AfterStates, \n",
    "                  rewards = CumulativeRewards,\n",
    "                  done = Dones)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    print(\"Agent epsilon: \", AC._epsilon)\n",
    "    plt.figure()\n",
    "    x = [(n + 1) * 50 for n in range(len(win_pct))]\n",
    "    y = (100*np.array(win_pct)).astype('int')\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win percentage of last 100 episodes')\n",
    "    plt.savefig('tensorflow_random.pdf')\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage:  1.0\n",
      "Agent epsilon:  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example policy: \n",
      " [0.25 0.25 0.22 0.22 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.\n",
      " 0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "win_pct = []\n",
    "AC._epsilon = 0\n",
    "for i in range(100):\n",
    "\n",
    "    wins = []\n",
    "\n",
    "    for _ in range(50):\n",
    "\n",
    "        env = backgammon()\n",
    "\n",
    "        #states = []\n",
    "        #currstates = []\n",
    "        #afterstates = []\n",
    "        #rewards = []\n",
    "        #afterstates.append([])\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                #currstates.append(env.board)\n",
    "                #afterstates.append(env.board)\n",
    "\n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                #rewards.append(reward)\n",
    "                #states.append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            reward = -1\n",
    "                            break\n",
    "\n",
    "        #afterstates.append(new_board)\n",
    "        #afterstates = afterstates[2:]\n",
    "\n",
    "        #Dones = np.zeros(len(states))\n",
    "        #Dones[-1] = 1\n",
    "\n",
    "        #States = np.vstack(states)\n",
    "        #CurrStates = np.vstack(currstates)\n",
    "        #AfterStates = np.vstack(afterstates)\n",
    "        #Rewards = AC.get_cumulative_rewards(rewards)\n",
    "\n",
    "\n",
    "        #AC.update(states = States, \n",
    "        #          rewards = Rewards, \n",
    "        #          currstates = CurrStates,\n",
    "        #          afterstates = AfterStates, \n",
    "        #          done = Dones)\n",
    "\n",
    "        wins.append(int(reward == 1))\n",
    "\n",
    "    win_pct.append(np.mean(wins))\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    print(\"Agent epsilon: \", AC._epsilon)\n",
    "    plt.figure()\n",
    "    x = [(n + 1) * 50 for n in range(len(win_pct))]\n",
    "    y = (100 * np.array(win_pct)).astype('int')\n",
    "    plt.plot(x, y) \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win percentage of last 100 episodes')\n",
    "    plt.ylim(0, 100)\n",
    "    #plt.savefig('tensorflow_random.pdf')\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "#AC = ActorCritic(sess = s, entropy = 0.01, learning_rate = 0.001, gamma = 0.99,\n",
    "#                epsilon = 1, epsdecay = 0.999)\n",
    "for i in range(10):\n",
    "\n",
    "    wins = []\n",
    "\n",
    "    for _ in range(50):\n",
    "\n",
    "        env = backgammon()\n",
    "\n",
    "        states = []\n",
    "        currstates = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                currstates.append(env.board)\n",
    "                afterstates.append(env.board)\n",
    "\n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                states.append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "\n",
    "        afterstates.append(new_board)\n",
    "        afterstates = afterstates[2:]\n",
    "\n",
    "        Dones = np.zeros(len(states))\n",
    "        Dones[-1] = 1\n",
    "\n",
    "        States = np.vstack(states)\n",
    "        CurrStates = np.vstack(currstates)\n",
    "        AfterStates = np.vstack(afterstates)\n",
    "        Rewards = AC.get_cumulative_rewards(rewards)\n",
    "\n",
    "\n",
    "        AC.update(states = States, \n",
    "                  rewards = Rewards, \n",
    "                  currstates = CurrStates,\n",
    "                  afterstates = AfterStates, \n",
    "                  done = Dones)\n",
    "\n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "\n",
    "    win_pct.append(np.mean(wins))\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    print(\"Agent epsilon: \", AC._epsilon)\n",
    "    plt.figure()\n",
    "    x = [(n + 1) * 50 for n in range(len(win_pct))]\n",
    "    y = (100 * np.array(win_pct)).astype('int')\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win percentage of last 100 episodes')\n",
    "    plt.ylim(0, 100)\n",
    "    #plt.savefig('tensorflow_random.pdf')\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spila við random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage:  [0.65 0.   0.   0.   0.   0.   0.   0.   0.   0.  ]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example policy: \n",
      " [0.18 0.18 0.09 0.09 0.07 0.07 0.04 0.04 0.02 0.02 0.02 0.02 0.02 0.02\n",
      " 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.01 0.   0.   0.   0.   0.\n",
      " 0.   0.   0.   0.   0.   0.   0.  ]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-db723e46d411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     31\u001b[0m                     \u001b[0mafterstates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAC\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_boards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0mnew_board\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-3139f60e684e>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_states\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "win_pct = np.zeros([10, 10])\n",
    "\n",
    "for j in range(10):\n",
    "    AC = ActorCritic(sess = s, entropy = 0.01, learning_rate = 1e-3, gamma = 0.99)\n",
    "    for i in range(10):\n",
    "\n",
    "        wins = []\n",
    "\n",
    "        for _ in range(100):\n",
    "\n",
    "            env = backgammon()\n",
    "\n",
    "            states = []\n",
    "            currstates = []\n",
    "            afterstates = []\n",
    "            rewards = []\n",
    "            afterstates.append([])\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                dice = B.roll_dice()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                    possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                    n_actions = len(possible_moves)\n",
    "\n",
    "                    if n_actions == 0:\n",
    "                        break\n",
    "\n",
    "                    currstates.append(env.board)\n",
    "                    afterstates.append(env.board)\n",
    "\n",
    "                    action = AC.sample_action(possible_boards)\n",
    "                    new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    states.append(new_board)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                if not done:\n",
    "                    dice = B.roll_dice()\n",
    "\n",
    "                    for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                            new_board, reward, done = env.make_move(dice)\n",
    "                            if done:\n",
    "                                rewards[-1] = -1\n",
    "                                break\n",
    "\n",
    "            afterstates.append(new_board)\n",
    "            afterstates = afterstates[2:]\n",
    "\n",
    "            Dones = np.zeros(len(states))\n",
    "            Dones[-1] = 1\n",
    "\n",
    "            States = np.vstack(states)\n",
    "            CurrStates = np.vstack(currstates)\n",
    "            AfterStates = np.vstack(afterstates)\n",
    "            Rewards = AC.get_cumulative_rewards(rewards)\n",
    "\n",
    "\n",
    "            AC.update(states = States, \n",
    "                      rewards = Rewards, \n",
    "                      currstates = CurrStates,\n",
    "                      afterstates = AfterStates, \n",
    "                      done = Dones)\n",
    "\n",
    "            wins.append(int(rewards[-1] == 1))\n",
    "\n",
    "        win_pct[i, j] = np.mean(wins)\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"Win percentage: \", win_pct[-1])\n",
    "        plt.figure()\n",
    "        x = [(n + 1) * 100 for n in range(10)]\n",
    "        y = (100 * win_pct).astype('int')\n",
    "        plt.plot(x, y, 'o-', color = \"b\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Win percentage of last 100 episodes')\n",
    "        plt.savefig('tensorflow_random.pdf')\n",
    "        plt.show()\n",
    "        print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure()\n",
    "x = [(n + 1) * 100 for n in range(10)]\n",
    "y = (100 * np.mean(win_pct, axis = 1)).astype('int')\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win percentage of last 100 episodes')\n",
    "plt.savefig('tensorflow_random.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Þjálfa Player2 (Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = PG.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                states.append(new_board)\n",
    "                afterstates.append(new_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                            \n",
    "        afterstates.append(new_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        Dones = np.zeros(len(states))\n",
    "        Dones[-1] = 1\n",
    "        \n",
    "        States = np.vstack(states)\n",
    "        Rewards = PG.get_cumulative_rewards(rewards)\n",
    "        AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        PG.update(states = States, \n",
    "                      rewards = Rewards, \n",
    "                      afterstates = AfterStates, \n",
    "                      done = Dones)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    print(PG.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG vs. Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        #states = []\n",
    "        #afterstates = []\n",
    "        #rewards = []\n",
    "        #afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = PG.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                #rewards.append(reward)\n",
    "                #states.append(old_board)\n",
    "                #afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                \n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            reward = 0\n",
    "                            break\n",
    "                            \n",
    "        #afterstates.append(old_board)\n",
    "        #afterstates = afterstates[2:]\n",
    "        \n",
    "        #Dones = np.zeros(len(states))\n",
    "        #Dones[-1] = 1\n",
    "        \n",
    "        #States = np.vstack(states)\n",
    "        #Rewards = player.get_cumulative_rewards(rewards)\n",
    "        #AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        #player.update(states = States, \n",
    "        #              rewards = Rewards, \n",
    "        #              afterstates = AfterStates, \n",
    "        #              done = Dones)\n",
    "        \n",
    "        wins.append(reward)\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keppni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        #states = []\n",
    "        #afterstates = []\n",
    "        #rewards = []\n",
    "        #afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                #rewards.append(reward)\n",
    "                #states.append(old_board)\n",
    "                #afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            reward = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                            \n",
    "        #afterstates.append(old_board)\n",
    "        #afterstates = afterstates[2:]\n",
    "        \n",
    "        #Dones = np.zeros(len(states))\n",
    "        #Dones[-1] = 1\n",
    "        \n",
    "        #States = np.vstack(states)\n",
    "        #Rewards = player.get_cumulative_rewards(rewards)\n",
    "        #AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        #player.update(states = States, \n",
    "        #              rewards = Rewards, \n",
    "        #              afterstates = AfterStates, \n",
    "        #              done = Dones)\n",
    "        \n",
    "        wins.append(int(reward == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Þjálfa AC á PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                states.append(new_board)\n",
    "                afterstates.append(new_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                            \n",
    "        afterstates.append(new_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        Dones = np.zeros(len(states))\n",
    "        Dones[-1] = 1\n",
    "        States = np.vstack(states)\n",
    "        CumulativeRewards = AC.get_cumulative_rewards(rewards)\n",
    "        AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        AC.update(states = States, \n",
    "                      rewards = CumulativeRewards,\n",
    "                      afterstates = AfterStates, \n",
    "                      done = Dones)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    print(\"Example policy: \", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prófa batch training (Virkar illa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    States = []\n",
    "    AfterStates = []\n",
    "    Rewards = []\n",
    "    Dones = []\n",
    "    \n",
    "    for _ in range(50):\n",
    "        \n",
    "        env = backgammon()\n",
    "        done = False\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        \n",
    "        afterstates.append([])\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                states.append(old_board)\n",
    "                afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                \n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        old_state, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "        afterstates.append(old_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        dones = np.zeros(len(rewards))\n",
    "        dones[-1] = 1\n",
    "        rewards = player.get_cumulative_rewards(rewards)\n",
    "        \n",
    "        States.append(states)\n",
    "        AfterStates.append(afterstates)\n",
    "        Rewards.append(rewards)\n",
    "        Dones.append(dones)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    #Rewards = np.concatenate(Rewards).flatten()\n",
    "    #States = np.vstack(States)\n",
    "    #AfterStates = np.vstack(AfterStates)\n",
    "    #Dones = np.concatenate(Dones).flatten()\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    \n",
    "    for r, s, a, d in zip(Rewards, States, AfterStates, Dones):\n",
    "        player.update(states = np.vstack(s), \n",
    "                      rewards = r, \n",
    "                      afterstates = np.vstack(a), \n",
    "                      done = d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Agent (Virkar illa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvActorCritic:\n",
    "    def __init__(self, gamma = 0.99, learning_rate = 1e-3, entropy = 0.1):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "        self._states2D = tf.expand_dims(self._states, 2)\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"afterstates\")\n",
    "        self._afterstates2D = tf.expand_dims(self._afterstates, 2)\n",
    "        self._done = tf.placeholder(\"float32\", (None, ), name = \"dones\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, ), name = \"rewards\")\n",
    "        \n",
    "        # Actor\n",
    "        self.network = keras.models.Sequential()\n",
    "        self.network.add(L.InputLayer(input_shape = (29, 1)))\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 2))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 4))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 8))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5,filters = 16))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 32))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 8))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Flatten())\n",
    "        self.network.add(L.Dense(128))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Dense(1))\n",
    "                         \n",
    "        \n",
    "        # Predictions\n",
    "        \n",
    "        ## Critic\n",
    "        self._state_values = self.network(self._states2D)\n",
    "        self._afterstate_values = self.network(self._afterstates2D) * (1 - self._done)\n",
    "        self._target_state_values = self._cumulative_rewards + self._gamma * self._afterstate_values * (1 - self._done)\n",
    "        \n",
    "        self._advantage = self._cumulative_rewards + self._gamma * self._afterstate_values - self._state_values\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self.network(self._states2D)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_entropy = -tf.reduce_sum(self._actor_policy * self._actor_log_policy)\n",
    "        \n",
    "        # Losses\n",
    "        self._critic_loss = tf.reduce_mean((self._state_values - tf.stop_gradient(self._target_state_values)))\n",
    "        self._actor_loss = -tf.reduce_sum(self._actor_log_policy * tf.stop_gradient(self._advantage))\n",
    "        self._actor_loss -= entropy * self._actor_entropy\n",
    "        \n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self._update = self._optimizer.minimize(self._critic_loss + self._actor_loss)\n",
    "        \n",
    "        self._s = tf.InteractiveSession()\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._states: states})).flatten()\n",
    "        \n",
    "        return np.random.choice(np.arange(len(probs)), p = probs)\n",
    "    \n",
    "    def update(self, states, rewards, afterstates, done):\n",
    "        self._s.run(self._update, \n",
    "                    ({self._states: states, \n",
    "                      self._afterstates: afterstates,\n",
    "                      self._done: done,\n",
    "                      self._cumulative_rewards: rewards}))\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "        r = 0.\n",
    "        for i, reward in enumerate(reversed(rewards)):\n",
    "            r += reward\n",
    "            R[-(i + 1)] = r\n",
    "            r *= self._gamma\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "PG = PolicyGradient(sess = s, entropy = 0.1, learning_rate=1e-4, gamma = 0.9)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        reward_vector = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                reward_vector.append(np.zeros(len(possible_boards)))\n",
    "                states.append(possible_boards)\n",
    "                actions.append(action)\n",
    "                afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        old_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                                     \n",
    "                            \n",
    "        afterstates.append(old_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        CumulativeRewards = PG.get_cumulative_rewards(rewards)\n",
    "        \n",
    "        for States, Reward, Reward_vector, Action in zip(states, CumulativeRewards, reward_vector, actions):\n",
    "            Reward_vector[Action] = Reward\n",
    "            \n",
    "            States = np.vstack(States)\n",
    "            Reward_vector = np.array(Reward_vector)\n",
    "            \n",
    "            PG.update(rewards = Reward_vector, states = States, afterstates = 0, done = 0)\n",
    "        \n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", PG.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sér network fyrir Actor og Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma = 0.99):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"afterstates\")\n",
    "        self._done = tf.placeholder(\"float32\", (None, ), name = \"dones\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, ), name = \"rewards\")\n",
    "        \n",
    "        # Actor\n",
    "        self.actor = keras.models.Sequential()\n",
    "        self.actor.add(L.Dense(32))\n",
    "        self.actor.add(L.LeakyReLU())\n",
    "        self.actor.add(L.Dense(64))\n",
    "        self.actor.add(L.LeakyReLU())\n",
    "        self.actor.add(L.Dense(32))\n",
    "        self.actor.add(L.LeakyReLU())\n",
    "        self.actor.add(L.Dense(1))\n",
    "        \n",
    "        # Critic\n",
    "        \n",
    "        self.critic = keras.models.Sequential()\n",
    "        self.critic.add(L.Dense(32))\n",
    "        self.critic.add(L.LeakyReLU())\n",
    "        self.critic.add(L.Dense(64))\n",
    "        self.critic.add(L.LeakyReLU())\n",
    "        self.critic.add(L.Dense(32))\n",
    "        self.critic.add(L.LeakyReLU())\n",
    "        self.critic.add(L.Dense(1))\n",
    "        \n",
    "        # Losses and logits\n",
    "        \n",
    "        ## Critic\n",
    "        self._state_values = self.critic(self._states)\n",
    "        self._afterstate_values = self.critic(self._afterstates) * (1 - self._done)\n",
    "        self._advantage = self._cumulative_rewards + self._gamma * self._afterstate_values - self._state_values\n",
    "        \n",
    "    \n",
    "        self._target_state_values = self._cumulative_rewards + self._gamma * self._afterstate_values * (1 - self._done)\n",
    "        \n",
    "        self._critic_loss = tf.reduce_mean((self._state_values - tf.stop_gradient(self._target_state_values)))\n",
    "        self._critic_optimizer = tf.train.AdamOptimizer()\n",
    "        self._critic_update = self._critic_optimizer.minimize(self._critic_loss)\n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self.actor(self._states)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        \n",
    "        self._actor_loss = -tf.reduce_sum(self._actor_log_policy * tf.stop_gradient(self._advantage))\n",
    "        self._actor_optimizer = tf.train.AdamOptimizer()\n",
    "        self._actor_update = self._actor_optimizer.minimize(self._actor_loss)\n",
    "        \n",
    "        self._s = tf.InteractiveSession()\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._states: states})).flatten()\n",
    "        \n",
    "        return np.random.choice(np.arange(len(probs)), p = probs)\n",
    "    \n",
    "    def update(self, boards, rewards, afterstates, done):\n",
    "        self._s.run([self._actor_update, self._critic_update], \n",
    "                    ({self._states: boards, \n",
    "                      self._afterstates: afterstates,\n",
    "                      self._done: done,\n",
    "                      self._cumulative_rewards: rewards}))\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "        r = 0.\n",
    "        for i, reward in enumerate(reversed(rewards)):\n",
    "            r += reward\n",
    "            R[-(i + 1)] = r\n",
    "            r *= self._gamma\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
