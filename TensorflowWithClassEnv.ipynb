{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import Backgammon as B\n",
    "import agent as A\n",
    "import flipped_agent as FA\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import deque\n",
    "import time\n",
    "import os.path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backgammon:\n",
    "    def __init__(self):\n",
    "        self.board = B.init_board()\n",
    "            \n",
    "    def reset(self):\n",
    "        self.board = B.init_board()\n",
    "        self.done = False\n",
    "    \n",
    "    def legal_moves(self, dice, player):\n",
    "        moves, boards = B.legal_moves(board = self.board, dice = dice, player = player)\n",
    "        if len(boards) == 0:\n",
    "            return [], []\n",
    "        boards = np.vstack(boards)\n",
    "        return moves, boards\n",
    "    \n",
    "    def swap_player(self):\n",
    "        self.board = FA.flip_board(board_copy=np.copy(self.board))\n",
    "    \n",
    "    # oppents random move\n",
    "    def make_move(self, dice):\n",
    "        moves, _ = self.legal_moves(dice, -1)\n",
    "        if len(moves) == 0:\n",
    "            return self.step([], -1)\n",
    "        move = moves[np.random.randint(len(moves))]\n",
    "        return self.step(move, -1)\n",
    "    \n",
    "    def step(self, move, player = 1):\n",
    "        old_board = np.copy(self.board)\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                self.board = B.update_board(board = self.board, move = m, player = player)\n",
    "        reward = 0\n",
    "        self.done = False\n",
    "        if self.iswin():\n",
    "            reward = player\n",
    "            self.done = True\n",
    "        return old_board, np.copy(self.board), reward, self.done\n",
    "    \n",
    "    def symbolic_step(self, move):\n",
    "        board = np.copy(self.board)\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                board = B.update_board(board = board, move = m, player = 1)\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if B.game_over(board):\n",
    "            reward = 1\n",
    "            done = True\n",
    "        return board, reward, self.done\n",
    "        \n",
    "    def iswin(self):\n",
    "        return B.game_over(self.board)\n",
    "        \n",
    "    def render(self):\n",
    "        B.pretty_print(self.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PolicyGrad Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PolicyGradient:\n",
    "    def __init__(self, sess, gamma = 0.99, learning_rate = 1e-3, entropy = 0.1,\n",
    "                epsilon = 1, epsdecay = 0.999):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        self._epsilon = epsilon\n",
    "        self._epsdecay = epsdecay\n",
    "        \n",
    "        self._states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"afterstates\")\n",
    "        self._done = tf.placeholder(\"float32\", (None, ), name = \"dones\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, ), name = \"rewards\")\n",
    "        \n",
    "        # Actor\n",
    "        self.network = keras.models.Sequential()\n",
    "        self.network.add(L.Dense(32))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Dense(32))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Dense(1))\n",
    "        \n",
    "        # Predictions\n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self.network(self._states)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_entropy = -tf.reduce_sum(self._actor_policy * self._actor_log_policy)\n",
    "        \n",
    "        # Losses\n",
    "        self._actor_loss = -tf.reduce_sum(self._actor_log_policy * self._cumulative_rewards)\n",
    "        self._actor_loss -= entropy * self._actor_entropy\n",
    "        \n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self._update = self._optimizer.minimize(self._actor_loss)\n",
    "        \n",
    "        self._s = sess\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._states: states})).flatten()\n",
    "        if np.random.uniform() < self._epsilon:\n",
    "            action = np.random.choice(np.arange(len(probs)), p = probs)\n",
    "        else:\n",
    "            action = np.argmax(probs)\n",
    "            \n",
    "        return action\n",
    "    \n",
    "    def update(self, states, rewards, afterstates, done):\n",
    "        self._s.run(self._update, \n",
    "                    ({self._states: states, \n",
    "                      self._cumulative_rewards: rewards}))\n",
    "        self._epsilon *= self._epsdecay\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "        r = 0.\n",
    "        for i, reward in enumerate(reversed(rewards)):\n",
    "            r += reward\n",
    "            R[-(i + 1)] = r\n",
    "            r *= self._gamma\n",
    "        return R\n",
    "    \n",
    "    def ExamplePolicy(self):\n",
    "        _, st = B.legal_moves(B.init_board(), B.roll_dice(), 1)\n",
    "        \n",
    "        out = np.round(self._s.run(self._actor_policy, ({self._states: st})) * 100)/100\n",
    "        out = out.flatten()\n",
    "        out.sort()\n",
    "        return out[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic (Shared network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, sess, gamma = 0.99, learning_rate = 0.001, entropy = 0.01, \n",
    "                 read_file = True, save_path = \"ACModelTF.h5\"):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        self._iters = 0\n",
    "        self._path = save_path\n",
    "        \n",
    "        self._currstates = tf.placeholder(\"float32\", (None, 29), name = \"CurrentStates\")\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"AfterStates\")\n",
    "        self._is_terminal = tf.placeholder(\"float32\", (None, 1), name = \"IsTerminal\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, 1), name = \"Rewards\")\n",
    "        self._s = sess\n",
    "        \n",
    "        # Network\n",
    "        if os.path.isfile(self._path) and read_file:\n",
    "            self._network = keras.models.load_model(self._path)\n",
    "        else:\n",
    "            self._network = keras.models.Sequential()\n",
    "            self._network.add(L.Dropout(0.2))\n",
    "            self._network.add(L.Dense(32, \n",
    "                                      kernel_regularizer = keras.regularizers.l2(0.01),\n",
    "                                      kernel_initializer = keras.initializers.glorot_normal(seed=None)))\n",
    "            self._network.add(L.LeakyReLU())\n",
    "            self._network.add(L.Dense(64,\n",
    "                                      kernel_regularizer = keras.regularizers.l2(0.01),\n",
    "                                      kernel_initializer = keras.initializers.glorot_normal(seed=None)))\n",
    "            self._network.add(L.LeakyReLU())\n",
    "            self._network.add(L.Dense(32, \n",
    "                                      kernel_regularizer = keras.regularizers.l2(0.01),\n",
    "                                      kernel_initializer = keras.initializers.glorot_normal(seed=None)))\n",
    "            self._network.add(L.LeakyReLU())\n",
    "            self._network.add(L.Dense(1))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Predictions\n",
    "        ## Critic\n",
    "        self._current_state_values = tf.nn.tanh(self._network(self._currstates))\n",
    "        self._afterstate_values = tf.nn.tanh(self._network(self._afterstates)) * (1 - self._is_terminal)\n",
    "        \n",
    "        self._target_state_values = self._cumulative_rewards\n",
    "        self._target_state_values += self._gamma * self._afterstate_values * (1 - self._is_terminal)\n",
    "        \n",
    "        self._advantage = self._target_state_values - self._current_state_values\n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self._network(self._afterstates)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_entropy = -tf.reduce_sum(self._actor_policy * self._actor_log_policy)\n",
    "        \n",
    "        # Losses\n",
    "        self._critic_loss = -tf.reduce_sum(tf.stop_gradient(self._advantage) * self._current_state_values)\n",
    "        self._actor_loss = -tf.reduce_sum(tf.stop_gradient(self._advantage) * self._actor_log_policy)\n",
    "        self._actor_loss -= entropy * self._actor_entropy\n",
    "        \n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self._update = self._optimizer.minimize(self._actor_loss + self._critic_loss)\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._afterstates: states})).flatten()\n",
    "            \n",
    "        return np.random.choice(np.arange(len(probs)), p = probs)\n",
    "    \n",
    "    def update(self, currstates, afterstates, cumulative_rewards, is_terminal):\n",
    "        \n",
    "        self._s.run(self._update, \n",
    "                    ({self._currstates: currstates,\n",
    "                      self._afterstates: afterstates, \n",
    "                      self._is_terminal: is_terminal,\n",
    "                      self._cumulative_rewards: cumulative_rewards}))\n",
    "        self._iters += 1\n",
    "        \n",
    "        if self._iters % 1000 == 0:\n",
    "            self.save_network()\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        reward = rewards[-1]\n",
    "        i = np.arange(len(rewards))[::-1]\n",
    "        R = reward * (self._gamma ** i)\n",
    "        return R\n",
    "    \n",
    "    def ExamplePolicy(self):\n",
    "        _, st = B.legal_moves(B.init_board(), B.roll_dice(), 1)\n",
    "        \n",
    "        out = np.round(self._s.run(self._actor_policy, ({self._afterstates: st})) * 100)/100\n",
    "        out = out.flatten()\n",
    "        out.sort()\n",
    "        return out[::-1]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(str(self._network.summary()))\n",
    "    \n",
    "    def save_network(self):\n",
    "        self._network.save(self._path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.type = \"Random\"\n",
    "    def sample_action(self, states):\n",
    "        action = np.random.randint(len(states))\n",
    "        return(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlayRandomAgent(player, n_games = 100):\n",
    "    wins = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "\n",
    "        env = backgammon()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, new_board, reward, done = env.step(possible_moves[action])\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        old_board, new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            reward = -1\n",
    "                            break\n",
    "\n",
    "        wins.append(float(reward == 1))\n",
    "        \n",
    "    return(np.mean(wins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelfPlay(player, n_games = 1000, test_each = 100, test_games = 10):\n",
    "    win_pct = []\n",
    "    for i in range(n_games):\n",
    "        env = backgammon()\n",
    "\n",
    "        active = 0\n",
    "\n",
    "        currstates = [[], []]\n",
    "        afterstates = [[], []]\n",
    "        rewards = [[], []]\n",
    "        is_terminal = [[], []]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                rewards[active].append(reward)\n",
    "                currstates[active].append(old_board)\n",
    "                afterstates[active].append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    rewards[(active + 1) % 2][-1] = -1\n",
    "\n",
    "                    is_terminal[active].append(1)\n",
    "                    is_terminal[(active + 1) % 2][-1] = 1\n",
    "                    break\n",
    "                else:\n",
    "                    is_terminal[active].append(0)\n",
    "                env.swap_player()\n",
    "                active = (active + 1) % 2\n",
    "        CurrStates = np.vstack([np.vstack(player_data) for player_data in currstates])\n",
    "        AfterStates = np.vstack([np.vstack(player_data) for player_data in afterstates])\n",
    "        CumulativeRewards = np.vstack([np.vstack(player.get_cumulative_rewards(player_data)) for player_data in rewards])\n",
    "        IsTerminal = np.vstack([np.vstack(player_data) for player_data in is_terminal])\n",
    "\n",
    "\n",
    "        player.update(currstates = CurrStates, \n",
    "                      afterstates = AfterStates, \n",
    "                      cumulative_rewards = CumulativeRewards,\n",
    "                      is_terminal = IsTerminal)\n",
    "        \n",
    "        if (i + 1) % test_each == 0:\n",
    "            outcome = PlayRandomAgent(player, n_games = test_games)\n",
    "            win_pct.append(outcome)\n",
    "            example = player.ExamplePolicy()\n",
    "            clear_output(True)\n",
    "            print(\"Win percentage: %.5f\" % (win_pct[-1]))\n",
    "            print(\"Example policy: \\n\", example)\n",
    "            \n",
    "            plt.figure()\n",
    "            x = [(n + 1) * test_each for n in range(len(win_pct))]\n",
    "            y = (100*np.array(win_pct)).astype('int')\n",
    "            plt.plot(x, y)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Win percentage of last 100 episodes')\n",
    "            plt.ylim(0, 100)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = ActorCritic(sess = s, entropy = 0, learning_rate = 0.0003, gamma = 0.99)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Batchgammon:\n",
    "    def __init__(self, n_envs = 10):\n",
    "        self.envs = [backgammon() for _ in range(n_envs)]\n",
    "        self.n_envs = n_envs\n",
    "        \n",
    "        self.currstates = [[[], []] for i in range(self.n_envs)]\n",
    "        self.afterstates = [[[], []] for i in range(self.n_envs)]\n",
    "        self.rewards = [[[], []] for i in range(self.n_envs)]\n",
    "        self.is_terminal = [[[], []] for i in range(self.n_envs)]\n",
    "        \n",
    "        self.active = np.zeros(n_envs)\n",
    "        self.dice = []\n",
    "        \n",
    "        \n",
    "        self.playedgames = 0\n",
    "        \n",
    "    def reset(self):\n",
    "        return np.array([env.reset() for env in self.envs])\n",
    "    \n",
    "    def legal_moves(self, dice, player):\n",
    "        info =  [B.legal_moves(board = env.board, dice = d, player = p)\n",
    "                for env, d, p in zip(self.envs, dice, player)]\n",
    "        \n",
    "        moves, boards = [item for item in zip(*info)]\n",
    "        \n",
    "        boards = [np.vstack(board) for board in boards]\n",
    "        \n",
    "        return moves, boards\n",
    "    \n",
    "    def step(self, player):\n",
    "        self.dice = [B.roll_dice() for i in range(n_envs)]\n",
    "        \n",
    "        \n",
    "        info =  [B.legal_moves(board = env.board, dice = d, player = p)\n",
    "                for env, d, p in zip(self.envs, self.dice, self.active)]\n",
    "        \n",
    "        moves, boards = [item for item in zip(*info)]\n",
    "        \n",
    "        boards = [np.vstack(board) for board in boards]\n",
    "        action = player.sample_action(possible_boards)\n",
    "        results = [env.step(a) for env, a, d in zip(self.envs, actions, self.dice)]\n",
    "        \n",
    "        old_boards, new_boards, rewards, done = map(np.array, zip(*results))\n",
    "        \n",
    "        self.active = (self.active + 1) % 2\n",
    "        \n",
    "        for i in range(len(self.envs)):\n",
    "            self.currstates[i][player[i]].append(old_boards[i])\n",
    "            self.afterstates[i][player[i]].append(new_boards[i])\n",
    "            self.rewards[i][player[i]].append(rewards[i])\n",
    "            self.isterminal[i][player[i]].append(done[i])\n",
    "            \n",
    "            if done[i]:\n",
    "                CurrStates = np.vstack([np.vstack(player_data) \n",
    "                                        for player_data in self.currstates[i]])\n",
    "                AfterStates = np.vstack([np.vstack(player_data) \n",
    "                                         for player_data in self.afterstates[i]])\n",
    "                CumulativeRewards = np.vstack([np.vstack(player.get_cumulative_rewards(player_data)) \n",
    "                                               for player_data in self.rewards[i]])\n",
    "                IsTerminal = np.vstack([np.vstack(player_data) \n",
    "                                        for player_data in self.is_terminal[i]])\n",
    "\n",
    "\n",
    "                player.update(currstates = CurrStates, \n",
    "                              afterstates = AfterStates, \n",
    "                              cumulative_rewards = CumulativeRewards,\n",
    "                              is_terminal = IsTerminal)\n",
    "        \n",
    "                self.envs[i] = backgammon()\n",
    "                self.active[i] = 0\n",
    "                self.playedgames += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AsyncSelfPlay(player, n_envs = 10, n_games = 1000, test_each = 100, test_games = 10):\n",
    "    win_pct = []\n",
    "    envs = [backgammon() for i in range(n_envs)]\n",
    "    played_games = 0\n",
    "    currstates = [[[], []] for i in range(n_envs)]\n",
    "    afterstates = [[[], []] for i in range(n_envs)]\n",
    "    rewards = [[[], []] for i in range(n_envs)]\n",
    "    is_terminal = [[[], []] for i in range(n_envs)]\n",
    "\n",
    "    active = np.zeros(n_envs, dtype = \"int\")\n",
    "\n",
    "    while played_games < n_games:\n",
    "        for i in range(n_envs):\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = envs[i].legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, new_board, reward, done = envs[i].step(possible_moves[action], player = 1)\n",
    "                \n",
    "                currstates[i][active[i]].append(old_board)\n",
    "                rewards[i][active[i]].append(reward)\n",
    "                afterstates[i][active[i]].append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    rewards[i][(active[i] + 1) % 2][-1] = -1\n",
    "\n",
    "                    is_terminal[i][active[i]].append(1)\n",
    "                    is_terminal[i][(active[i] + 1) % 2][-1] = 1\n",
    "                    \n",
    "                    CurrStates = np.vstack([np.vstack(player_data) \n",
    "                                            for player_data in currstates[i]])\n",
    "                    AfterStates = np.vstack([np.vstack(player_data) \n",
    "                                             for player_data in afterstates[i]])\n",
    "                    CumulativeRewards = np.vstack([np.vstack(player.get_cumulative_rewards(player_data)) \n",
    "                                                   for player_data in rewards[i]])\n",
    "                    IsTerminal = np.vstack([np.vstack(player_data) \n",
    "                                            for player_data in is_terminal[i]])\n",
    "\n",
    "\n",
    "                    player.update(currstates = CurrStates, \n",
    "                                  afterstates = AfterStates, \n",
    "                                  cumulative_rewards = CumulativeRewards,\n",
    "                                  is_terminal = IsTerminal)\n",
    "                    \n",
    "                    envs[i] = backgammon()\n",
    "                    currstates[i] = [[], []]\n",
    "                    afterstates[i] = [[], []]\n",
    "                    rewards[i] = [[], []]\n",
    "                    is_terminal[i] = [[], []]\n",
    "                    \n",
    "                    played_games += 1\n",
    "                    \n",
    "                    break\n",
    "                else:\n",
    "                    is_terminal[i][active[i]].append(0)\n",
    "                envs[i].swap_player()\n",
    "                active[i] = (active[i] + 1) % 2\n",
    "            \n",
    "\n",
    "        if (played_games + 1) % test_each == 0:\n",
    "            outcome = PlayRandomAgent(player, n_games = test_games)\n",
    "            win_pct.append(outcome)\n",
    "            example = player.ExamplePolicy()\n",
    "            clear_output(True)\n",
    "            print(\"Win percentage: %.5f\" % (win_pct[-1]))\n",
    "            print(\"Example policy: \\n\", example)\n",
    "\n",
    "            plt.figure()\n",
    "            x = [(n + 1) * test_each for n in range(len(win_pct))]\n",
    "            y = (100*np.array(win_pct)).astype('int')\n",
    "            plt.plot(x, y)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Win percentage of last 100 episodes')\n",
    "            plt.ylim(0, 100)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = ActorCritic(sess = s, entropy = 0.1, learning_rate = 0.0005, gamma = 0.99, read_file = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_2 (Dropout)          (None, 29)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                960       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,185\n",
      "Trainable params: 5,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage: 0.95000\n",
      "Example policy: \n",
      " [0.09 0.09 0.06 0.06 0.06 0.06 0.05 0.05 0.04 0.04 0.03 0.03 0.03 0.03\n",
      " 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.01 0.01 0.01\n",
      " 0.01 0.01 0.01]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "AsyncSelfPlay(player = AC, n_envs = 50, test_each = 100, test_games = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stakt episode\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spila við sjálft sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(50):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        currstates = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                currstates.append(env.board)\n",
    "                \n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                afterstates.append(new_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = AC.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "        \n",
    "        IsTerminal = np.zeros(len(currstates))\n",
    "        IsTerminal[-1] = 1\n",
    "        CumulativeRewards = AC.get_cumulative_rewards(rewards)\n",
    "        CurrStates = np.vstack(currstates)\n",
    "        AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        AC.update(currstates = CurrStates,\n",
    "                  afterstates = AfterStates, \n",
    "                  rewards = CumulativeRewards,\n",
    "                  is_terminal = IsTerminal)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    print(\"Agent epsilon: \", AC._epsilon)\n",
    "    plt.figure()\n",
    "    x = [(n + 1) * 50 for n in range(len(win_pct))]\n",
    "    y = (100*np.array(win_pct)).astype('int')\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win percentage of last 100 episodes')\n",
    "    plt.savefig('tensorflow_random.pdf')\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "AC._epsilon = 0\n",
    "for i in range(100):\n",
    "\n",
    "    wins = []\n",
    "\n",
    "    for _ in range(50):\n",
    "\n",
    "        env = backgammon()\n",
    "\n",
    "        #states = []\n",
    "        #currstates = []\n",
    "        #afterstates = []\n",
    "        #rewards = []\n",
    "        #afterstates.append([])\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                #currstates.append(env.board)\n",
    "                #afterstates.append(env.board)\n",
    "\n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                #rewards.append(reward)\n",
    "                #states.append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            reward = -1\n",
    "                            break\n",
    "\n",
    "        #afterstates.append(new_board)\n",
    "        #afterstates = afterstates[2:]\n",
    "\n",
    "        #Dones = np.zeros(len(states))\n",
    "        #Dones[-1] = 1\n",
    "\n",
    "        #States = np.vstack(states)\n",
    "        #CurrStates = np.vstack(currstates)\n",
    "        #AfterStates = np.vstack(afterstates)\n",
    "        #Rewards = AC.get_cumulative_rewards(rewards)\n",
    "\n",
    "\n",
    "        #AC.update(states = States, \n",
    "        #          rewards = Rewards, \n",
    "        #          currstates = CurrStates,\n",
    "        #          afterstates = AfterStates, \n",
    "        #          done = Dones)\n",
    "\n",
    "        wins.append(int(reward == 1))\n",
    "\n",
    "    win_pct.append(np.mean(wins))\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    print(\"Agent epsilon: \", AC._epsilon)\n",
    "    plt.figure()\n",
    "    x = [(n + 1) * 50 for n in range(len(win_pct))]\n",
    "    y = (100 * np.array(win_pct)).astype('int')\n",
    "    plt.plot(x, y) \n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win percentage of last 100 episodes')\n",
    "    plt.ylim(0, 100)\n",
    "    #plt.savefig('tensorflow_random.pdf')\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "#AC = ActorCritic(sess = s, entropy = 0.01, learning_rate = 0.001, gamma = 0.99,\n",
    "#                epsilon = 1, epsdecay = 0.999)\n",
    "for i in range(10):\n",
    "\n",
    "    wins = []\n",
    "\n",
    "    for _ in range(50):\n",
    "\n",
    "        env = backgammon()\n",
    "\n",
    "        states = []\n",
    "        currstates = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                currstates.append(env.board)\n",
    "                afterstates.append(env.board)\n",
    "\n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                rewards.append(reward)\n",
    "                states.append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "\n",
    "        afterstates.append(new_board)\n",
    "        afterstates = afterstates[2:]\n",
    "\n",
    "        Dones = np.zeros(len(states))\n",
    "        Dones[-1] = 1\n",
    "\n",
    "        States = np.vstack(states)\n",
    "        CurrStates = np.vstack(currstates)\n",
    "        AfterStates = np.vstack(afterstates)\n",
    "        Rewards = AC.get_cumulative_rewards(rewards)\n",
    "\n",
    "\n",
    "        AC.update(states = States, \n",
    "                  rewards = Rewards, \n",
    "                  currstates = CurrStates,\n",
    "                  afterstates = AfterStates, \n",
    "                  done = Dones)\n",
    "\n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "\n",
    "    win_pct.append(np.mean(wins))\n",
    "\n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    print(\"Agent epsilon: \", AC._epsilon)\n",
    "    plt.figure()\n",
    "    x = [(n + 1) * 50 for n in range(len(win_pct))]\n",
    "    y = (100 * np.array(win_pct)).astype('int')\n",
    "    plt.plot(x, y)\n",
    "    plt.xlabel('Episode')\n",
    "    plt.ylabel('Win percentage of last 100 episodes')\n",
    "    plt.ylim(0, 100)\n",
    "    #plt.savefig('tensorflow_random.pdf')\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spila við random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = np.zeros([10, 10])\n",
    "\n",
    "for j in range(10):\n",
    "    AC = ActorCritic(sess = s, entropy = 0.01, learning_rate = 1e-3, gamma = 0.99)\n",
    "    for i in range(10):\n",
    "\n",
    "        wins = []\n",
    "\n",
    "        for _ in range(100):\n",
    "\n",
    "            env = backgammon()\n",
    "\n",
    "            states = []\n",
    "            currstates = []\n",
    "            afterstates = []\n",
    "            rewards = []\n",
    "            afterstates.append([])\n",
    "            done = False\n",
    "\n",
    "            while not done:\n",
    "                dice = B.roll_dice()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                    possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                    n_actions = len(possible_moves)\n",
    "\n",
    "                    if n_actions == 0:\n",
    "                        break\n",
    "\n",
    "                    currstates.append(env.board)\n",
    "                    afterstates.append(env.board)\n",
    "\n",
    "                    action = AC.sample_action(possible_boards)\n",
    "                    new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                    rewards.append(reward)\n",
    "                    states.append(new_board)\n",
    "\n",
    "                    if done:\n",
    "                        break\n",
    "\n",
    "                if not done:\n",
    "                    dice = B.roll_dice()\n",
    "\n",
    "                    for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                            new_board, reward, done = env.make_move(dice)\n",
    "                            if done:\n",
    "                                rewards[-1] = -1\n",
    "                                break\n",
    "\n",
    "            afterstates.append(new_board)\n",
    "            afterstates = afterstates[2:]\n",
    "\n",
    "            Dones = np.zeros(len(states))\n",
    "            Dones[-1] = 1\n",
    "\n",
    "            States = np.vstack(states)\n",
    "            CurrStates = np.vstack(currstates)\n",
    "            AfterStates = np.vstack(afterstates)\n",
    "            Rewards = AC.get_cumulative_rewards(rewards)\n",
    "\n",
    "\n",
    "            AC.update(states = States, \n",
    "                      rewards = Rewards, \n",
    "                      currstates = CurrStates,\n",
    "                      afterstates = AfterStates, \n",
    "                      done = Dones)\n",
    "\n",
    "            wins.append(int(rewards[-1] == 1))\n",
    "\n",
    "        win_pct[i, j] = np.mean(wins)\n",
    "\n",
    "        clear_output(True)\n",
    "        print(\"Win percentage: \", win_pct[-1])\n",
    "        plt.figure()\n",
    "        x = [(n + 1) * 100 for n in range(10)]\n",
    "        y = (100 * win_pct).astype('int')\n",
    "        plt.plot(x, y, 'o-', color = \"b\")\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Win percentage of last 100 episodes')\n",
    "        plt.savefig('tensorflow_random.pdf')\n",
    "        plt.show()\n",
    "        print(\"Example policy: \\n\", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "x = [(n + 1) * 100 for n in range(10)]\n",
    "y = (100 * np.mean(win_pct, axis = 1)).astype('int')\n",
    "plt.plot(x, y, 'o-')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Win percentage of last 100 episodes')\n",
    "plt.savefig('tensorflow_random.pdf')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Þjálfa Player2 (Policy Gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(10):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = PG.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                states.append(new_board)\n",
    "                afterstates.append(new_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                            \n",
    "        afterstates.append(new_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        Dones = np.zeros(len(states))\n",
    "        Dones[-1] = 1\n",
    "        \n",
    "        States = np.vstack(states)\n",
    "        Rewards = PG.get_cumulative_rewards(rewards)\n",
    "        AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        PG.update(states = States, \n",
    "                      rewards = Rewards, \n",
    "                      afterstates = AfterStates, \n",
    "                      done = Dones)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    print(PG.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PG vs. Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        #states = []\n",
    "        #afterstates = []\n",
    "        #rewards = []\n",
    "        #afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = PG.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                #rewards.append(reward)\n",
    "                #states.append(old_board)\n",
    "                #afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                \n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            reward = 0\n",
    "                            break\n",
    "                            \n",
    "        #afterstates.append(old_board)\n",
    "        #afterstates = afterstates[2:]\n",
    "        \n",
    "        #Dones = np.zeros(len(states))\n",
    "        #Dones[-1] = 1\n",
    "        \n",
    "        #States = np.vstack(states)\n",
    "        #Rewards = player.get_cumulative_rewards(rewards)\n",
    "        #AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        #player.update(states = States, \n",
    "        #              rewards = Rewards, \n",
    "        #              afterstates = AfterStates, \n",
    "        #              done = Dones)\n",
    "        \n",
    "        wins.append(reward)\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keppni"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(5):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        #states = []\n",
    "        #afterstates = []\n",
    "        #rewards = []\n",
    "        #afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                #rewards.append(reward)\n",
    "                #states.append(old_board)\n",
    "                #afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            reward = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                            \n",
    "        #afterstates.append(old_board)\n",
    "        #afterstates = afterstates[2:]\n",
    "        \n",
    "        #Dones = np.zeros(len(states))\n",
    "        #Dones[-1] = 1\n",
    "        \n",
    "        #States = np.vstack(states)\n",
    "        #Rewards = player.get_cumulative_rewards(rewards)\n",
    "        #AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        #player.update(states = States, \n",
    "        #              rewards = Rewards, \n",
    "        #              afterstates = AfterStates, \n",
    "        #              done = Dones)\n",
    "        \n",
    "        wins.append(int(reward == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Þjálfa AC á PG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(100):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                states.append(new_board)\n",
    "                afterstates.append(new_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                            \n",
    "        afterstates.append(new_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        Dones = np.zeros(len(states))\n",
    "        Dones[-1] = 1\n",
    "        States = np.vstack(states)\n",
    "        CumulativeRewards = AC.get_cumulative_rewards(rewards)\n",
    "        AfterStates = np.vstack(afterstates)\n",
    "        \n",
    "        \n",
    "        AC.update(states = States, \n",
    "                      rewards = CumulativeRewards,\n",
    "                      afterstates = AfterStates, \n",
    "                      done = Dones)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    print(\"Example policy: \", AC.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prófa batch training (Virkar illa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "\n",
    "\n",
    "for i in range(1000):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    States = []\n",
    "    AfterStates = []\n",
    "    Rewards = []\n",
    "    Dones = []\n",
    "    \n",
    "    for _ in range(50):\n",
    "        \n",
    "        env = backgammon()\n",
    "        done = False\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        rewards = []\n",
    "        \n",
    "        afterstates.append([])\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                states.append(old_board)\n",
    "                afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                \n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        old_state, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "        afterstates.append(old_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        dones = np.zeros(len(rewards))\n",
    "        dones[-1] = 1\n",
    "        rewards = player.get_cumulative_rewards(rewards)\n",
    "        \n",
    "        States.append(states)\n",
    "        AfterStates.append(afterstates)\n",
    "        Rewards.append(rewards)\n",
    "        Dones.append(dones)\n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    #Rewards = np.concatenate(Rewards).flatten()\n",
    "    #States = np.vstack(States)\n",
    "    #AfterStates = np.vstack(AfterStates)\n",
    "    #Dones = np.concatenate(Dones).flatten()\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    \n",
    "    for r, s, a, d in zip(Rewards, States, AfterStates, Dones):\n",
    "        player.update(states = np.vstack(s), \n",
    "                      rewards = r, \n",
    "                      afterstates = np.vstack(a), \n",
    "                      done = d)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Agent (Virkar illa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvActorCritic:\n",
    "    def __init__(self, gamma = 0.99, learning_rate = 1e-3, entropy = 0.1):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "        self._states2D = tf.expand_dims(self._states, 2)\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"afterstates\")\n",
    "        self._afterstates2D = tf.expand_dims(self._afterstates, 2)\n",
    "        self._done = tf.placeholder(\"float32\", (None, ), name = \"dones\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, ), name = \"rewards\")\n",
    "        \n",
    "        # Actor\n",
    "        self.network = keras.models.Sequential()\n",
    "        self.network.add(L.InputLayer(input_shape = (29, 1)))\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 2))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 4))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 8))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5,filters = 16))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 32))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Conv1D(kernel_size = 5, filters = 8))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Flatten())\n",
    "        self.network.add(L.Dense(128))\n",
    "        self.network.add(L.LeakyReLU())\n",
    "        self.network.add(L.Dense(1))\n",
    "                         \n",
    "        \n",
    "        # Predictions\n",
    "        \n",
    "        ## Critic\n",
    "        self._state_values = self.network(self._states2D)\n",
    "        self._afterstate_values = self.network(self._afterstates2D) * (1 - self._done)\n",
    "        self._target_state_values = self._cumulative_rewards + self._gamma * self._afterstate_values * (1 - self._done)\n",
    "        \n",
    "        self._advantage = self._cumulative_rewards + self._gamma * self._afterstate_values - self._state_values\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self.network(self._states2D)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_entropy = -tf.reduce_sum(self._actor_policy * self._actor_log_policy)\n",
    "        \n",
    "        # Losses\n",
    "        self._critic_loss = tf.reduce_mean((self._state_values - tf.stop_gradient(self._target_state_values)))\n",
    "        self._actor_loss = -tf.reduce_sum(self._actor_log_policy * tf.stop_gradient(self._advantage))\n",
    "        self._actor_loss -= entropy * self._actor_entropy\n",
    "        \n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self._update = self._optimizer.minimize(self._critic_loss + self._actor_loss)\n",
    "        \n",
    "        self._s = tf.InteractiveSession()\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._states: states})).flatten()\n",
    "        \n",
    "        return np.random.choice(np.arange(len(probs)), p = probs)\n",
    "    \n",
    "    def update(self, states, rewards, afterstates, done):\n",
    "        self._s.run(self._update, \n",
    "                    ({self._states: states, \n",
    "                      self._afterstates: afterstates,\n",
    "                      self._done: done,\n",
    "                      self._cumulative_rewards: rewards}))\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "        r = 0.\n",
    "        for i, reward in enumerate(reversed(rewards)):\n",
    "            r += reward\n",
    "            R[-(i + 1)] = r\n",
    "            r *= self._gamma\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "win_pct = []\n",
    "\n",
    "PG = PolicyGradient(sess = s, entropy = 0.1, learning_rate=1e-4, gamma = 0.9)\n",
    "\n",
    "for i in range(100):\n",
    "    \n",
    "    wins = []\n",
    "    \n",
    "    for _ in range(10):\n",
    "        \n",
    "        env = backgammon()\n",
    "        \n",
    "        states = []\n",
    "        afterstates = []\n",
    "        actions = []\n",
    "        rewards = []\n",
    "        reward_vector = []\n",
    "        afterstates.append([])\n",
    "        done = False\n",
    "        \n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                \n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "                \n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "                    \n",
    "                    \n",
    "                action = AC.sample_action(possible_boards)\n",
    "                new, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                \n",
    "                rewards.append(reward)\n",
    "                reward_vector.append(np.zeros(len(possible_boards)))\n",
    "                states.append(possible_boards)\n",
    "                actions.append(action)\n",
    "                afterstates.append(old_board)\n",
    "                \n",
    "                if done:\n",
    "                    break\n",
    "                    \n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "                env.swap_player()\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                        n_actions = len(possible_moves)\n",
    "                        \n",
    "                        if n_actions == 0:\n",
    "                            break\n",
    "                        \n",
    "                        action = PG.sample_action(possible_boards)\n",
    "                        \n",
    "                        old_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "                        \n",
    "                        if done:\n",
    "                            rewards[-1] = -1\n",
    "                            break\n",
    "                            \n",
    "                env.swap_player()\n",
    "                                     \n",
    "                            \n",
    "        afterstates.append(old_board)\n",
    "        afterstates = afterstates[2:]\n",
    "        \n",
    "        CumulativeRewards = PG.get_cumulative_rewards(rewards)\n",
    "        \n",
    "        for States, Reward, Reward_vector, Action in zip(states, CumulativeRewards, reward_vector, actions):\n",
    "            Reward_vector[Action] = Reward\n",
    "            \n",
    "            States = np.vstack(States)\n",
    "            Reward_vector = np.array(Reward_vector)\n",
    "            \n",
    "            PG.update(rewards = Reward_vector, states = States, afterstates = 0, done = 0)\n",
    "        \n",
    "        \n",
    "        wins.append(int(rewards[-1] == 1))\n",
    "    \n",
    "    win_pct.append(np.mean(wins))\n",
    "    \n",
    "    clear_output(True)\n",
    "    print(\"Win percentage: \", win_pct[-1])\n",
    "    plt.plot(win_pct)\n",
    "    plt.show()\n",
    "    print(\"Example policy: \\n\", PG.ExamplePolicy())\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sér network fyrir Actor og Critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, gamma = 0.99):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        \n",
    "        self._states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"afterstates\")\n",
    "        self._done = tf.placeholder(\"float32\", (None, ), name = \"dones\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, ), name = \"rewards\")\n",
    "        \n",
    "        # Actor\n",
    "        self.actor = keras.models.Sequential()\n",
    "        self.actor.add(L.Dense(32))\n",
    "        self.actor.add(L.LeakyReLU())\n",
    "        self.actor.add(L.Dense(64))\n",
    "        self.actor.add(L.LeakyReLU())\n",
    "        self.actor.add(L.Dense(32))\n",
    "        self.actor.add(L.LeakyReLU())\n",
    "        self.actor.add(L.Dense(1))\n",
    "        \n",
    "        # Critic\n",
    "        \n",
    "        self.critic = keras.models.Sequential()\n",
    "        self.critic.add(L.Dense(32))\n",
    "        self.critic.add(L.LeakyReLU())\n",
    "        self.critic.add(L.Dense(64))\n",
    "        self.critic.add(L.LeakyReLU())\n",
    "        self.critic.add(L.Dense(32))\n",
    "        self.critic.add(L.LeakyReLU())\n",
    "        self.critic.add(L.Dense(1))\n",
    "        \n",
    "        # Losses and logits\n",
    "        \n",
    "        ## Critic\n",
    "        self._state_values = self.critic(self._states)\n",
    "        self._afterstate_values = self.critic(self._afterstates) * (1 - self._done)\n",
    "        self._advantage = self._cumulative_rewards + self._gamma * self._afterstate_values - self._state_values\n",
    "        \n",
    "    \n",
    "        self._target_state_values = self._cumulative_rewards + self._gamma * self._afterstate_values * (1 - self._done)\n",
    "        \n",
    "        self._critic_loss = tf.reduce_mean((self._state_values - tf.stop_gradient(self._target_state_values)))\n",
    "        self._critic_optimizer = tf.train.AdamOptimizer()\n",
    "        self._critic_update = self._critic_optimizer.minimize(self._critic_loss)\n",
    "        \n",
    "        ## Actor\n",
    "        self._actor_logits = self.actor(self._states)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        \n",
    "        self._actor_loss = -tf.reduce_sum(self._actor_log_policy * tf.stop_gradient(self._advantage))\n",
    "        self._actor_optimizer = tf.train.AdamOptimizer()\n",
    "        self._actor_update = self._actor_optimizer.minimize(self._actor_loss)\n",
    "        \n",
    "        self._s = tf.InteractiveSession()\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._states: states})).flatten()\n",
    "        \n",
    "        return np.random.choice(np.arange(len(probs)), p = probs)\n",
    "    \n",
    "    def update(self, boards, rewards, afterstates, done):\n",
    "        self._s.run([self._actor_update, self._critic_update], \n",
    "                    ({self._states: boards, \n",
    "                      self._afterstates: afterstates,\n",
    "                      self._done: done,\n",
    "                      self._cumulative_rewards: rewards}))\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        rewards = np.array(rewards)\n",
    "        R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "        r = 0.\n",
    "        for i, reward in enumerate(reversed(rewards)):\n",
    "            r += reward\n",
    "            R[-(i + 1)] = r\n",
    "            r *= self._gamma\n",
    "        return R"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
