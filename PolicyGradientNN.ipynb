{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Backgammon as B\n",
    "import agent as A\n",
    "import flipped_agent as FA\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, gamma = 0.99):\n",
    "   \n",
    "    rewards = np.array(rewards)\n",
    "    R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "    r = 0.\n",
    "    for i, reward in enumerate(reversed(rewards)):\n",
    "        r += reward\n",
    "        R[-(i + 1)] = r\n",
    "        r *= gamma\n",
    "        \n",
    "    return R\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_states, _actions, _rewards):\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states: _states, \n",
    "                actions: _actions, \n",
    "                cumulative_rewards: _cumulative_rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skilgreina inputs í model\n",
    "states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "actions = tf.placeholder(\"int32\", name = \"action_ids\")\n",
    "cumulative_rewards = tf.placeholder(\"float32\", name = \"cumulative_rewards\")\n",
    "\n",
    "# Skilgreina model (arkitektúrinn skiptir litlu máli þangað til að þjálfunin gengur)\n",
    "model = keras.models.Sequential()\n",
    "model.add(L.Dense(32, activation = \"relu\"))\n",
    "model.add(L.Dense(64, activation = \"relu\"))\n",
    "model.add(L.Dense(1))\n",
    "\n",
    "\n",
    "\n",
    "logits = model(states)\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)\n",
    "\n",
    "get_action_prob = lambda s: policy.eval({states: s})\n",
    "\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy,indices)\n",
    "\n",
    "\n",
    "J = tf.reduce_mean(log_policy_for_actions * cumulative_rewards)\n",
    "entropy = -tf.reduce_sum(tf.multiply(policy, log_policy), 1, name=\"entropy\")\n",
    "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "loss = -J - 0.1 * entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many or too few pieces on board!\n",
      "Error at game step  3\n"
     ]
    }
   ],
   "source": [
    "# Spila leikinn\n",
    "boards, moves, rewards = [], [], []\n",
    "    \n",
    "board = B.init_board()\n",
    "player = 1\n",
    "\n",
    "k = 1 # Halda utan um hvenær ég fæ villu\n",
    "error = False\n",
    "\n",
    "\n",
    "'''\n",
    "Pælingin hér er að spila einn leik til enda og geyma öll boards og actions. \n",
    "Reikna svo eligibility trace og update'a modelið með öllu episodeinu.\n",
    "'''\n",
    "\n",
    "while True:\n",
    "    dice = B.roll_dice()\n",
    "\n",
    "    for i in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "        legal_moves, legal_boards = B.legal_moves(board, dice, 1)\n",
    "        legal_boards = np.array([board for board in legal_boards])\n",
    "\n",
    "        if len(legal_moves) == 0:\n",
    "            break\n",
    "\n",
    "        probs = get_action_prob(legal_boards)\n",
    "        n_actions = probs.shape[0]\n",
    "        probs = probs.reshape(n_actions)\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "        action = np.random.choice(np.arange(0, n_actions), \n",
    "                             p = probs)\n",
    "\n",
    "        move = legal_moves[action]\n",
    "\n",
    "        next_board = B.update_board(board = board, move = move.T, player = 1)\n",
    "\n",
    "        #record session history to train later\n",
    "        boards.append(next_board)\n",
    "        moves.append(move)\n",
    "        board = next_board\n",
    "\n",
    "        if B.game_over(board):\n",
    "            rewards.append(1)\n",
    "            break\n",
    "        else:\n",
    "            rewards.append(0)\n",
    "            \n",
    "    board = FA.flip_board(board)\n",
    "    k += 1\n",
    "    if B.check_for_error(board):\n",
    "        Error = True\n",
    "        print(\"Error at game step \", k)\n",
    "        break\n",
    "        \n",
    "if not Error:        \n",
    "    train_step(boards, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
