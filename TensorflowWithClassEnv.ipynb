{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Backgammon as B\n",
    "import agent as A\n",
    "import flipped_agent as FA\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "from collections import deque\n",
    "import time\n",
    "import os.path\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class backgammon:\n",
    "    def __init__(self):\n",
    "        self.board = B.init_board()\n",
    "            \n",
    "    def reset(self):\n",
    "        self.board = B.init_board()\n",
    "        self.done = False\n",
    "    \n",
    "    def legal_moves(self, dice, player):\n",
    "        moves, boards = B.legal_moves(board = self.board, dice = dice, player = player)\n",
    "        if len(boards) == 0:\n",
    "            return [], []\n",
    "        boards = np.vstack(boards)\n",
    "        return moves, boards\n",
    "    \n",
    "    def swap_player(self):\n",
    "        self.board = FA.flip_board(board_copy=np.copy(self.board))\n",
    "    \n",
    "    # oppents random move\n",
    "    def make_move(self, dice):\n",
    "        moves, _ = self.legal_moves(dice, -1)\n",
    "        if len(moves) == 0:\n",
    "            return self.step([], -1)\n",
    "        move = moves[np.random.randint(len(moves))]\n",
    "        return self.step(move, -1)\n",
    "    \n",
    "    def step(self, move, player = 1):\n",
    "        old_board = np.copy(self.board)\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                self.board = B.update_board(board = self.board, move = m, player = player)\n",
    "        reward = 0\n",
    "        self.done = False\n",
    "        if self.iswin():\n",
    "            reward = player\n",
    "            self.done = True\n",
    "        return old_board, np.copy(self.board), reward, self.done\n",
    "    \n",
    "    def symbolic_step(self, move):\n",
    "        board = np.copy(self.board)\n",
    "        if len(move) != 0:\n",
    "            for m in move:\n",
    "                board = B.update_board(board = board, move = m, player = 1)\n",
    "        reward = 0\n",
    "        done = False\n",
    "        if B.game_over(board):\n",
    "            reward = 1\n",
    "            done = True\n",
    "        return board, reward, self.done\n",
    "        \n",
    "    def iswin(self):\n",
    "        return B.game_over(self.board)\n",
    "        \n",
    "    def render(self):\n",
    "        B.pretty_print(self.board)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic (Shared network)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ActorCritic:\n",
    "    def __init__(self, gamma = 0.99, learning_rate = 0.001, entropy = 0.01, \n",
    "                 read_file = True, save_path = \"/AgentData/AC_BGJ\"):\n",
    "        \n",
    "        self._gamma = gamma\n",
    "        self._iters = 0\n",
    "        self._path = save_path\n",
    "        \n",
    "        self._currstates = tf.placeholder(\"float32\", (None, 29), name = \"CurrentStates\")\n",
    "        self._afterstates = tf.placeholder(\"float32\", (None, 29), name = \"AfterStates\")\n",
    "        self._is_terminal = tf.placeholder(\"float32\", (None, 1), name = \"IsTerminal\")\n",
    "        self._cumulative_rewards = tf.placeholder(\"float32\", (None, 1), name = \"Rewards\")\n",
    "        \n",
    "        # Network\n",
    "        self._s = tf.Session()\n",
    "        self._network = keras.models.Sequential()\n",
    "        self._network.add(L.Dropout(0.2))\n",
    "        self._network.add(L.Dense(32, \n",
    "                                  kernel_regularizer = keras.regularizers.l2(0.01),\n",
    "                                  kernel_initializer = keras.initializers.glorot_normal(seed=None)))\n",
    "        self._network.add(L.LeakyReLU())\n",
    "        self._network.add(L.Dense(64,\n",
    "                                  kernel_regularizer = keras.regularizers.l2(0.01),\n",
    "                                  kernel_initializer = keras.initializers.glorot_normal(seed=None)))\n",
    "        self._network.add(L.LeakyReLU())\n",
    "        self._network.add(L.Dense(32, \n",
    "                                  kernel_regularizer = keras.regularizers.l2(0.01),\n",
    "                                  kernel_initializer = keras.initializers.glorot_normal(seed=None)))\n",
    "        self._network.add(L.LeakyReLU())\n",
    "        self._network.add(L.Dense(1))\n",
    "\n",
    "        # Predictions\n",
    "        ## Critic\n",
    "        self._current_state_values = tf.nn.tanh(self._network(self._currstates))\n",
    "        self._afterstate_values = tf.nn.tanh(self._network(self._afterstates)) * (1 - self._is_terminal)\n",
    "\n",
    "        self._target_state_values = self._cumulative_rewards\n",
    "        self._target_state_values += self._gamma * self._afterstate_values * (1 - self._is_terminal)\n",
    "\n",
    "        self._advantage = self._target_state_values - self._current_state_values\n",
    "\n",
    "        ## Actor\n",
    "        self._actor_logits = self._network(self._afterstates)\n",
    "        self._actor_policy = tf.nn.softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_log_policy = tf.nn.log_softmax(self._actor_logits, axis = 0)\n",
    "        self._actor_entropy = -tf.reduce_sum(self._actor_policy * self._actor_log_policy)\n",
    "\n",
    "        # Losses\n",
    "        self._critic_loss = -tf.reduce_sum(tf.stop_gradient(self._advantage) * self._current_state_values)\n",
    "        self._actor_loss = -tf.reduce_sum(tf.stop_gradient(self._advantage) * self._actor_log_policy)\n",
    "        self._actor_loss -= entropy * self._actor_entropy\n",
    "\n",
    "        self._optimizer = tf.train.AdamOptimizer(learning_rate)\n",
    "        self._update = self._optimizer.minimize(self._actor_loss + self._critic_loss)\n",
    "        self._s.run(tf.global_variables_initializer())\n",
    "        \n",
    "        self._saver = tf.train.Saver()\n",
    "        if os.path.isfile(self._path + \".index\") and read_file:\n",
    "            self._saver = tf.train.import_meta_graph(self._path)\n",
    "            self._saver.restore(self._s, tf.train.latest_checkpoint(\"./\"))\n",
    "            \n",
    "    def __delete__(self):\n",
    "        self._s.close()\n",
    "        \n",
    "        \n",
    "    def sample_action(self, states):\n",
    "        probs = self._s.run(self._actor_policy, ({self._afterstates: states})).flatten()\n",
    "            \n",
    "        return np.random.choice(np.arange(len(probs)), p = probs)\n",
    "    \n",
    "    def update(self, currstates, afterstates, cumulative_rewards, is_terminal):\n",
    "        \n",
    "        self._s.run(self._update, \n",
    "                    ({self._currstates: currstates,\n",
    "                      self._afterstates: afterstates, \n",
    "                      self._is_terminal: is_terminal,\n",
    "                      self._cumulative_rewards: cumulative_rewards}))\n",
    "        self._iters += 1\n",
    "        \n",
    "        if self._iters % 1000 == 0:\n",
    "            self.save_network()\n",
    "        \n",
    "    def get_cumulative_rewards(self, rewards):\n",
    "        reward = rewards[-1]\n",
    "        i = np.arange(len(rewards))[::-1]\n",
    "        R = reward * (self._gamma ** i)\n",
    "        return R\n",
    "    \n",
    "    def ExamplePolicy(self):\n",
    "        _, st = B.legal_moves(B.init_board(), B.roll_dice(), 1)\n",
    "        \n",
    "        out = np.round(self._s.run(self._actor_policy, ({self._afterstates: st})) * 100)/100\n",
    "        out = out.flatten()\n",
    "        out.sort()\n",
    "        return out[::-1]\n",
    "    \n",
    "    def __str__(self):\n",
    "        return(str(self._network.summary()))\n",
    "    \n",
    "    def save_network(self):\n",
    "        self._saver.save(self._s, \".\" + self._path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent:\n",
    "    def __init__(self):\n",
    "        self.type = \"Random\"\n",
    "    def sample_action(self, states):\n",
    "        action = np.random.randint(len(states))\n",
    "        return(action)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to play game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def PlayRandomAgent(player, n_games = 100):\n",
    "    wins = []\n",
    "\n",
    "    for _ in range(n_games):\n",
    "\n",
    "        env = backgammon()\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, new_board, reward, done = env.step(possible_moves[action])\n",
    "\n",
    "                if done:\n",
    "                    break\n",
    "\n",
    "            if not done:\n",
    "                dice = B.roll_dice()\n",
    "\n",
    "                for _ in range(1 + int(dice[0] == dice[1])):\n",
    "                        old_board, new_board, reward, done = env.make_move(dice)\n",
    "                        if done:\n",
    "                            reward = -1\n",
    "                            break\n",
    "\n",
    "        wins.append(float(reward == 1))\n",
    "        \n",
    "    return(np.mean(wins))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SelfPlay(player, n_games = 1000, test_each = 100, test_games = 10):\n",
    "    win_pct = []\n",
    "    for i in range(n_games):\n",
    "        env = backgammon()\n",
    "\n",
    "        active = 0\n",
    "\n",
    "        currstates = [[], []]\n",
    "        afterstates = [[], []]\n",
    "        rewards = [[], []]\n",
    "        is_terminal = [[], []]\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = env.legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, new_board, reward, done = env.step(possible_moves[action], player = 1)\n",
    "\n",
    "                rewards[active].append(reward)\n",
    "                currstates[active].append(old_board)\n",
    "                afterstates[active].append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    rewards[(active + 1) % 2][-1] = -1\n",
    "\n",
    "                    is_terminal[active].append(1)\n",
    "                    is_terminal[(active + 1) % 2][-1] = 1\n",
    "                    break\n",
    "                else:\n",
    "                    is_terminal[active].append(0)\n",
    "                env.swap_player()\n",
    "                active = (active + 1) % 2\n",
    "        CurrStates = np.vstack([np.vstack(player_data) for player_data in currstates])\n",
    "        AfterStates = np.vstack([np.vstack(player_data) for player_data in afterstates])\n",
    "        CumulativeRewards = np.vstack([np.vstack(player.get_cumulative_rewards(player_data)) for player_data in rewards])\n",
    "        IsTerminal = np.vstack([np.vstack(player_data) for player_data in is_terminal])\n",
    "\n",
    "\n",
    "        player.update(currstates = CurrStates, \n",
    "                      afterstates = AfterStates, \n",
    "                      cumulative_rewards = CumulativeRewards,\n",
    "                      is_terminal = IsTerminal)\n",
    "        \n",
    "        if (i + 1) % test_each == 0:\n",
    "            outcome = PlayRandomAgent(player, n_games = test_games)\n",
    "            win_pct.append(outcome)\n",
    "            example = player.ExamplePolicy()\n",
    "            clear_output(True)\n",
    "            print(\"Win percentage: %.5f\" % (win_pct[-1]))\n",
    "            print(\"Example policy: \\n\", example)\n",
    "            \n",
    "            plt.figure()\n",
    "            x = [(n + 1) * test_each for n in range(len(win_pct))]\n",
    "            y = (100*np.array(win_pct)).astype('int')\n",
    "            plt.plot(x, y)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Win percentage of last 100 episodes')\n",
    "            plt.ylim(0, 100)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def AsyncSelfPlay(player, n_envs = 10, n_games = 1000, test_each = 100, test_games = 10):\n",
    "    win_pct = []\n",
    "    envs = [backgammon() for i in range(n_envs)]\n",
    "    played_games = 0\n",
    "    currstates = [[[], []] for i in range(n_envs)]\n",
    "    afterstates = [[[], []] for i in range(n_envs)]\n",
    "    rewards = [[[], []] for i in range(n_envs)]\n",
    "    is_terminal = [[[], []] for i in range(n_envs)]\n",
    "\n",
    "    active = np.zeros(n_envs, dtype = \"int\")\n",
    "\n",
    "    while played_games < n_games:\n",
    "        for i in range(n_envs):\n",
    "            dice = B.roll_dice()\n",
    "            for _ in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "                possible_moves, possible_boards = envs[i].legal_moves(dice, 1)\n",
    "                n_actions = len(possible_moves)\n",
    "\n",
    "                if n_actions == 0:\n",
    "                    break\n",
    "\n",
    "                \n",
    "                action = player.sample_action(possible_boards)\n",
    "                old_board, new_board, reward, done = envs[i].step(possible_moves[action], player = 1)\n",
    "                \n",
    "                currstates[i][active[i]].append(old_board)\n",
    "                rewards[i][active[i]].append(reward)\n",
    "                afterstates[i][active[i]].append(new_board)\n",
    "\n",
    "                if done:\n",
    "                    rewards[i][(active[i] + 1) % 2][-1] = -1\n",
    "\n",
    "                    is_terminal[i][active[i]].append(1)\n",
    "                    is_terminal[i][(active[i] + 1) % 2][-1] = 1\n",
    "                    \n",
    "                    CurrStates = np.vstack([np.vstack(player_data) \n",
    "                                            for player_data in currstates[i]])\n",
    "                    AfterStates = np.vstack([np.vstack(player_data) \n",
    "                                             for player_data in afterstates[i]])\n",
    "                    CumulativeRewards = np.vstack([np.vstack(player.get_cumulative_rewards(player_data)) \n",
    "                                                   for player_data in rewards[i]])\n",
    "                    IsTerminal = np.vstack([np.vstack(player_data) \n",
    "                                            for player_data in is_terminal[i]])\n",
    "\n",
    "\n",
    "                    player.update(currstates = CurrStates, \n",
    "                                  afterstates = AfterStates, \n",
    "                                  cumulative_rewards = CumulativeRewards,\n",
    "                                  is_terminal = IsTerminal)\n",
    "                    \n",
    "                    envs[i] = backgammon()\n",
    "                    currstates[i] = [[], []]\n",
    "                    afterstates[i] = [[], []]\n",
    "                    rewards[i] = [[], []]\n",
    "                    is_terminal[i] = [[], []]\n",
    "                    \n",
    "                    played_games += 1\n",
    "                    \n",
    "                    break\n",
    "                else:\n",
    "                    is_terminal[i][active[i]].append(0)\n",
    "                envs[i].swap_player()\n",
    "                active[i] = (active[i] + 1) % 2\n",
    "            \n",
    "\n",
    "        if (played_games + 1) % test_each == 0:\n",
    "            outcome = PlayRandomAgent(player, n_games = test_games)\n",
    "            win_pct.append(outcome)\n",
    "            example = player.ExamplePolicy()\n",
    "            clear_output(True)\n",
    "            print(\"Win percentage: %.5f\" % (win_pct[-1]))\n",
    "            print(\"Example policy: \\n\", example)\n",
    "\n",
    "            plt.figure()\n",
    "            x = [(n + 1) * test_each for n in range(len(win_pct))]\n",
    "            y = (100*np.array(win_pct)).astype('int')\n",
    "            plt.plot(x, y)\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Win percentage of last 100 episodes')\n",
    "            plt.ylim(0, 100)\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC = ActorCritic(entropy = 0.1, learning_rate = 0.00005, gamma = 0.99, read_file = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AC._iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "AC.save_network()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dropout_2 (Dropout)          (None, 29)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 32)                960       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 64)                2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 32)                2080      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 32)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 5,185\n",
      "Trainable params: 5,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(AC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Win percentage: 0.60000\n",
      "Example policy: \n",
      " [0.08 0.08 0.07 0.07 0.06 0.06 0.04 0.04 0.04 0.04 0.04 0.04 0.03 0.03\n",
      " 0.03 0.03 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.02 0.01 0.01]\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEKCAYAAAAIO8L1AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4xLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvDW2N/gAAIABJREFUeJzt3Xl8nHW1+PHPydosbdMs3RuaKS2lQHchBRQEREBW2cErIveiF5BNroJXBK/+VFxBwAUvKFfZkc2FTQQRbAvd6UJpO2mbNGmbfW/W8/vjeSZN05nJM8lMZpKe9+s1r2SezMxzOmnmPN/tfEVVMcYYY/pKincAxhhjEpMlCGOMMUFZgjDGGBOUJQhjjDFBWYIwxhgTlCUIY4wxQcUsQYjIIyKyV0TW9zqWKyKvi8gW9+s497iIyM9FZKuIrBORhbGKyxhjjDexbEH8Djijz7HbgTdUdSbwhnsf4Exgpnu7FvhlDOMyxhjjQcwShKq+DdT0OXwe8Kj7/aPA+b2O/586lgE5IjIpVrEZY4zpX8oQn2+CqlYAqGqFiIx3j08BSns9rsw9VtH3BUTkWpxWBllZWYtmz54d24iNMWaEWblyZZWqFvT3uKFOEKFIkGNBa4Co6kPAQwCLFy/WFStWxDIuY4wZcURkh5fHDfUspj2BriP36173eBkwrdfjpgLlQxybMcaYXoY6QbwEXOV+fxXwYq/jn3dnMxUD9YGuKGOMMfERsy4mEXkCOBnIF5Ey4C7gB8DTInINsBO42H34X4GzgK1AC3B1rOIyxhjjTcwShKpeHuJHpwZ5rALXxyoWY4wxkbOV1MYYY4KyBGGMMSYoSxDGGGOCsgRhjDEmKEsQxhhjgrIEYYwxJihLEMYYY4KyBGGMMSYoSxDGGGOCsgRhjDEmKEsQxhhjgrIEYYwxJihLEMYYY4KKKEGISJKIjIlVMMYYYxJHvwlCRB4XkTEikgVsBDaLyH/FPjRjjDHx5KUFMUdVG4DzcTb2KQT+LaZRGWOMiTsvCSJVRFJxEsSLqtoBaGzDMsYYE29eEsSvge1AFvC2iBwGNMQyKGOMMfHX75ajqvpz4Oe9Du0QkU/GLiRjjDGJwMsg9QQReVhEXnbvzwGuinlkxhhj4spLF9PvgFeBye79j4CbYxWQMcaYxOAlQeSr6tNAN4CqdgJdMY3KGGNM3HlJEM0ikoc7c0lEioH6mEZljDEm7vodpAZuBV4CZojIu0ABcFFMozLGGBN3XmYxrRKRk4AjAAE2u2shjDHGjGAhE4SIfDbEj2aJCKr6XIxiMsYYkwDCtSDOcb+OB44H/u7e/yTwFmAJwhhjRrCQCUJVrwYQkT/j1GOqcO9PAh4cmvCMMcbEi5dZTNMDycG1B5gVo3iMMcYkCC+zmN4SkVeBJ3Cmul4GvBnTqIwxxsSdl1lMN4jIBcAn3EMPqerzsQ3LGGNMvHlpQQD8C+jEaUG8F7twjDHGJAovxfouwUkKFwGXAMtFxBbKGWPMCOelBfHfwMdUdS+AiBQAfwOejWVgxhhj4svLLKakQHJwVXt8njHGmGHMywf9KyLyqoh8QUS+APwFZ2/qARORW0Rkg4isF5EnRGSUiBSJyHIR2SIiT4lI2mDOYYwxZnD6TRCq+l84247OBebhzGL6+kBPKCJTgBuBxap6NJCMM3X2HuBnqjoTqAWuGeg5jDHGDJ6XQeos4EVVvRX4FdAlIqmDPG8KkCEiKUAmUAGcwv5xjUeB8wd5DmOMMYPgpYvpbSDdvfL/G3A1zi5zA6Kqu4AfAztxEkM9sBKoczcjAigDpgR7vohcKyIrRGRFZWXlQMMwxhjTDy8JQlS1BfgscL+qXgDMGegJRWQccB5QhLONaRZwZpCHarDnq+pDqrpYVRcXFBQMNAxjjDH98JQgRGQJcCXOADV4X2AXzGlAiapWuvtKPIdTLTbH7XICmAqUD+IcxhhjBslLgrgZuAN4XlU3iIiPwdVi2gkUi0imiAhwKrDRfc3AAryrgBcHcQ5jjDGD5KUW0z+Af/S678eZhTQgqrpcRJ4FVuGU71gNPITTOnlSRL7rHnt4oOcwxhgzeOF2lLtXVW8WkT8RZDxAVc8d6ElV9S7grj6H/cCxA31NY4wx0RWuBfF79+uPhyIQY4wxiSXcjnIr3a//cFc1z8ZpSWxW1fYhis8YY0yc9DsGISKfwVkgtw0QoEhEvqSqL8c6OGOMMfHjZbrqT4BPqupWABGZgTOgbAnCGGNGMC/TXPcGkoPLD+wN9WAzfPz0tc18/hHb/8mYofTbd0s449636e4OuhY4oXhpQWwQkb8CT+OMQVwMvC8inwVQ1ediGJ+JoWUlNbxXUkNNczu5WVY815ih8Od1FXy4u5GP9jYye+KYeIcTlpcWxChgD3AScDJQCeQC5wBnxywyE3O7alsBWO6vjnMkxhwamts6WVtaB8DSbYn/d+dlodzVQxGIGVodXd1U1DsJYpm/mjOPmRTniIwZ+VbuqKWzWxFx/u6uPqEo3iGF5aXc9ywReUNE1rv354rIN2Mfmomlirp9dCuIwFJrQRgzJJb6q0lJEs46ehLLS2oSfhzCSxfTb3BqMXUAqOo6nA1+zDBWVtsCwImH5/PRniaqmtriHJExI98yfzXzpuVwyuzx1LV08OHuxniHFJaXBJGpqn2nunQGfaQZNsrc8YeLF08DYLm/Jp7hGDPiNbV1sq6snmJfLsUz8oDEb717SRBV7toHBRCRi3A2+jHDWGltC8lJwulzJpCVlsyyBP+Pasxwt2J7DV3dyhJfPlNyMijMzUz4vzsv01yvx6m2OltEdgElOHtDmGGsrLaViWNGMSo1mY8V5Sb8lYwxw91SfzWpycKiw8YBsMSXx8vrK+jqVpKTJM7RBddvC0JV/ap6GlAAzFbVE1V1R+xDM7FUVtvC1HEZABT78ti6t4nKRhuHMCZWlm2rZv60HDLSkgEonpFLw75ONlU0xDmy0Lx0MQGgqs2qmtgjKsaz0ppWpuVmAs6VDJDwzd3hrL6lI94hmDhq3NfBB7vqe/7WwLkwg8T+u/OcIMzI0dbZxZ7GfT0tiKMmjyE7PcW6mWKkrLaFBd95jYfe3hbvUEycvL+9hm7dnxQAJo3NYHpeZkIvmLMEcQiqqNuHKkwd57QgUpKTOLYoN6GvZIazjeUNdCvc88pm3t9us8UORcv8NaQlJ7HQHX8IWDIjj/dKnMHrRBQ2QYjIbBH5uoj8XETuc78/cqiCM7ERmOIaaEEAFPty8Vc2s6dhX7zCGrH8Vc0ATBwzihseX0W1rTk55CzdVs38whxGpSYfcLzYl0djWycbyuvjFFl4IROEiHwdeBJnD4j3gPfd758QkduHJjwTC6XuIrnAGATAEl8+kNj9ocOVv7KJ/Ox0fv1vi6ht6eDmp9Yk/ApaEz31rR1sKD9w/CEg0cf/wrUgrgE+pqo/UNU/uLcf4Owbfc3QhGdioay2hZQkYcLo9J5jcyaPYfSolIT9jzqc+Sub8RVkcfSUsdx9zlH8c0sVD765tf8nmhHh/ZKDxx8Cxo8Zha8gK2HHIcIliG5gcpDjk9yfmWGqrLaVSTmjSEne/+tPThKOK8plma2ojrqSqmZmFGQBcPmx0zhv/mR+9reP+NfWqjhHZobCUn81aSlJLCjMCfrzYl8e72+vpbMr8T5WwyWIm4E3RORlEXnIvb0CvAHcNDThmVgorWlhak7mQceLfXmUVDWzu97GIaKlvqWD6uZ2ivKdBCEifO+CYyjKz+LGJ9ewt9He65Fumb+ahUHGHwKW+PJoautkfXnirYcImSBU9RVgFvBt4FXgNeBu4Aj3Z2aYKqttZVpuxkHHh8O87OFmW1UTAL787J5jWekp/OLKRTS1dXDjE6sTdgaLGby6lnY2VjT0jPEFc5wvF0jMv7v+prlqr1uX+zXx2kHGs30dXextbOuZ4trbnEljGJuRmrD9ocORv9KZweRzu5gCjpg4mu+cdzTL/DXc+7eP4hGaGQLvldSg6swSDGX86FEcPj47If/uQtZiEpHTgV8AW4Bd7uGpwOEicp2qvjYE8Zko21V38BTXgKQk4ViryxRVJVVNpCTJATPGAi5ePI33Smq4/+9bWXTYOE4+YnwcIjSxtNRfTXpKEvNDjD8ELPHl8cdVZXR0dZOanDjL08IV67sPOE1Vt/c+KCJFwF8BWw8xDAXWQAT7wALnP+rrG/ewq66VKTkHJ5Gh1trexe6GfT19+MONv7KZwtzMkH/0/3Pe0awrq+eWp9bw15s+zqSxsXnPVZXNe+K7B3JnVzdb9jZx5KTYxVDf2sGqnbVhHzMlJ4NZE0bHLIbelvlrWDx9HOkpwccfAop9efx+2Q4+2FXPwsJxYR87lMIliBSgLMjxXUBqbMIxsRbYKChYCwKclZ3gFBa7cNHUIYsrlB+8vIkn3y9l2R2nMi4rLd7hRCwwxTWUjLRkfvG5hZxz/zvc8Phqnry2OCZXkEv91Vzxm+U8/h/HcfyM0P3hsfTCmnJue2YtP7l4Xkz+bzW1dXLBg+/2LEwMRQR+d/WxnDSrIOox9Fbb3M6miga++qlZ/T42MA6xdFv1sEkQjwDvi8iTQKl7bBrObnIPxzowExtlta2kJgvjR48K+vMjJoxmXGYqy/zxTxD7Orp4fvUu2jq7eXHNLr6Q4Pv39tXdrZRUN3PSEeE/iGYUZPP9zx7DTU+u4cevbuaOs6LfOF+9sw6Af26piluCeGdLJQDffGE9x0wdG9WreFXljuc+YHt1M/ddNp/CEC1kBe744wfc8tQa/nLjiTFrsQEsL3G6agMXXeHkZ6cza0I2y/zVXP/Jw2MWU6TCzWL6PnAFzurpJcDx7vdXuj8zw1BpTQuTczJC1p9PShKOK8pLiHGI1zbuoWFfJ6NHpfDMymCN2cS2q66V9s5ufB66x86bP4Urjyvk12/7+dvGPVGPZaNbUjpeM2VUlaX+apb48shKT+a6x1bR3Ba9jSn/sHwnf1pbzldPP4Lz5k9hQeG4oLeFheN48MqF7Ovo4iuPr6YjhmsPlvlryEhNZu7U8OMPAUt8eazYXkt7Z+LMAwrbllXVTe5K6q+o6g3u9xuHKjgTfWW1rUwLMoOpt2JfLmW1rZTWtAxRVME9s6KUKTkZ3HLaLDaUN7AxAeeJhxPo6vA6fnLn2XM4avIYvvrM2qi/95vc925dWT1NUfxg9qqkqpk9DW18Zu4k7rtsAdsqm7jzhfWoDn6K7/pd9XznTxs5aVYB/3nSjH4ff/h4p8W2YkctP35t86DPH8rSbdUsnj6OtBRvXYbFvjxaO7r4YFddzGKK1IA6O0Xk5WgHYoZGWW1ryPGHgCUz4l+XqbyulXe2VnHhoqlcsGAKaclJPLOytP8nJhB/pbsGoiC7n0c6RqUm84srF9LdrdzwxOqoXUk2t3VSUt3MsUW5dHUrK+JQUTawQn/JjDxOODyfm0+dxXOrd/HU+4P7nda3dnDdY6vIy07jZ5fOJ8njzmw9LbZ/xKbFVt3UxuY9jUHLa4RynPvYRJruGq5Y38IQt0XA/CGM0URJa3sXVU1t/SaIWROyyc1Ki2vZjedX70IVLlo4lXFZaZw2ZzwvrilPqOZ3f0qqmhk9KoX8bO+D64flZfHDi+aytrSO77+8KSpxfLi7AVX4t+LDSE2WuHQfLvVXM350ek932w2nHM6Jh+fzrZc2DLhlqKp87dm1lNe18sAVC8iNcBJDLFtsy0ucv51IEkRuVhqzJ45OqHI34VoQ7wM/Bn7S5/ZjwFunmkkou+oOruIajIhQ7HP2h4hGF0CkVJVnVpRyXFEuhXlOrBcvmkZNczt//3DvkMczUP7KZnz5WYhEtt/wmcdM4uoTpvPbd7fz8gcVg45jg/sBvOiwccyfljPkH0CqyjJ/NcW+vJ73IjlJuPey+eRkpHL946to3Bf5jnu/fXc7r27Yw+1nzmbRYaEXooUyKjWZB6+IfosNnNZ3Zloyc6eOjeh5xb48Vuyooa2zK2qxDEa4BLEJ+JKqfrLvDbAqY8NQaZB9IEJZ4stjV10rpTWtsQ7rICt21LK9uoWLF0/rOfbxmfmMH53Os8Oom8lf2eS5e6mvO848knnTcvjas+vYUR1+2mZ/NpY3kJOZyqSxo1jiy2P9rvoBfSAP1LbKZiob2w6azZOfnc79ly9gZ00Lt//xg4guRlbtrOV7f93Ep+ZM4JoTBz67bXp+9FtsEBh/yI14yvKSGXns6+hmbWli7A8RLvq7w/z8K4M5qYjkiMizIvKhiGwSkSUikisir4vIFvdr4kwGHiH2bxQUvgUB8a3L9MyKUrLSkjnrmIk9x1KSk7hg4RTe3Fw5LArctbR3Ul6/z9MMpmDSUpJ48IoFJCUJ1z22in0dA7+i3FDewFGTx7gtwzy6unVId7YL/B8K1t1ynC+Pr54+i798UMHvl+3w9Hp1Le185fHVTBw7ih9fNC/iFlpf0W6xVTa2sWVvU9jyGqEcV5SLSOLUZQo3zfVZVQ06xK+qLwzyvPcBr6jqbGAeTmvlduANVZ2JUzHWNiWKsrKaFtJSkijITu/3sYePzyY/O23I+6tb2jv5y7oKPjN3EplpBy7TuXjRNLq6lRdW7wrx7MSxvcrpzhtoCwKcRP7TS+axobyB7/x5YJMHO7q62bynkTnu6uWFh40jLTlpSLuZlvqrmThmFNPzgl+YfPkTM/jkEQV898+bWFcWfgZPd7fy1afXUtnYxi+uXMjYzOis2Y1mi61n/UME4w8BOZlpHDlxTMIMVIdbKBcTIjIG+ATwBQBVbQfaReQ84GT3YY8CbwFfH+r4RrKy2lam5mR4mukhIhzny+sZhxjoVdoyf7WbbPpPSgB//WA3ze1dB3QvBRw+PpsFhTk8s6KM//i4z1NMqso/t1QxvzCHMaMG9mFSWtPC3sY2Fh3mvVHrd6u4DrZEyKlHTuBLJ/n49T/8HFuUy3nzp0T0fH9lM+2d3Rw12ekLH5WazPzCnCH7AFJVlvurOfHw/JC/r6Qk4aeXzOczP/8n1z22ii99whfy9TbtbuSND/fyP+cd5Xl9gRdpKUk8cPkCzr7/Ha57bBV//M/jQ5bn7s/SbdVkpSVz9JTIxh8Cin15/GH5Dv5v6XbC/Q8/tiiPIybGtmTIkCcIwAdUAr8VkXnASpz9JSaoagWAqlaISNDKZSJyLXAtQGFh4dBEPEKU1bYwxcP4Q8DJswr4y7oKnny/lMuPjfy9fmPTHq55dAVHThrD89d5+4N7ZkUp0/MyWRziw/jiRdP4xvMfsLasnvnT+v+AeGZFGV/74zqOn5HH7685LuQCwVDqWzq4/DfLaGjtYPW3Tvf8/EAV12jUkLrt9CNYtq2ae/+2JeIEEdjreM7k/fWPlvjyuP/vW6hv7WBsRmyr5mzd20RVU3u/q4nHZaXxwJUL+dz/LufOFzeEfewFC6bwb8WHRTNMwJm8cc+Fx/DlP6zi9Y17OGdesP3S+rfMX83HiiIffwg47cjxPPJuCd/q53347vlHxz9BiEi6qrb1dyzCcy4EvqKqy0XkPiLoTlLVh4CHABYvXmyF9CNQWtvKpyd7v6r57MKpvLS2nLte2sC8qTkHfMj0p6y2hVufXsuUnAw2VTTw7T9t4PufnRv2OTurW1heUsNtp88KebV59rxJ/M+fN/DsytJ+E8SmigbufHE903Iz+Ne2au57Ywu3eqiLE6Cq3Pbs2p6xmw3l9Z6vWv2VTUzJySAjbWBXob2lJidxzrzJfPcvm9hdv4+JY4OXSQlmY3kD6SlJB4yFLJmRx31vbOH9khpOmzNh0PGFE+iiDLcfQsDCwnGs+OZptLSHHm8RnOmggx13COW0IyeQnZ7CUn/1gBLE3oZ9bKts5pIgLWCvjj88n7V3nd7vKu/s9Nhf33tJcUs9HvOqDChT1eXu/WdxEsYeEZkE4H4dPvMZh4Hmtk5qmts9zWAKSE4SfnbpfMZlRjYVsb2zmxsedzbCeezfj+O6k2fwxHulPL86fLmMZ1eVIeIkplDGjErljKMm8tKa8rADt01tnVz/2CrGZqTy3H+ewEWLpnL/37fw9keVnv4NAA+/U8LrG/fwpZOcLo9IBg5LqsIX6YvUQCcNbChvYPbE0QdsLzt/Wg5pKUlDMr60zF/N5LGjgm5QFUxmWgr52ekhb3nZ6TFLDuBMhji2KHfAg8TLBrD+IZixGalh34f87PQBd4FFItxCuYnuorgMEVnQa6HcyUD/02BCUNXdQKmIHOEeOhXYCLwEXOUeuwp4caDnMAcL7APR3xqIvpypiAsjmor4g5c/ZE1pHT+8aC7T87O49VOzOLYol288t54texqDPqe7W/njyjJOPDyfyf2UGb948TQa9nXyWogVsL0Lt91/+QIKRqfznfOOZtb40dzy1BpPW6qu3FHLD17+kE8fNYHbz5gd0cbyqoq/sjmqJcoHspmTqrKxouGglt+o1GQWFY6L+UyZ7m5lmb+G4hl5Mf1Qj7ZiXy7+ymb2NEQ+W27ptmpGp6dwVASt7UQWrgXxaZxFcVM5cKHcrcA3BnnerwCPicg6nFXZ3wN+AHxKRLYAn3LvmygJrBSNpAURcGxRLredfoSnqYivrK/gkXdL+MLx0znrmEmAc1V2/+ULyExzirS1tB9cC2ipv5pdda1BB6f7WuLLY0pOBs+sCL4monfhtkD5goy0ZB68ciGtHV185YlVYTeIr2lu54bHVzE5J4MfutMoI9lYvrKpjca2zgFPcQ3GKaIY2WZOu+paqW/tYE6QbsViXx4bKxqoa2mPWox9bdnbRE1z+6CvpodaoDtsIAl0uTv+kJJAm/4MRrhpro+6i+K+oKqn9Food66qPjeYk6rqGlVdrKpzVfV8Va1V1WpVPVVVZ7pfE2e9+QhQFsEiuWC+9Akfp8wez3f+vDHkVMQd1c3817PrmDd1LHecNfuAn00YM4r7LlvA1somvhmkSNszK0oZMyqF0z30iSclCRcunMI7W6sorztwId8HZU7htpOPOLhwW6BI2/vba/nxa8G3+ezuVm59eg3VTe3ONEp3EDeSjeX3bzM68CmuwRT78thZ09LTGuxPoITFnCAb9CyZkYeqsyVmrCzd5qynHch0z3iaM3kMo0elRJwg9jTsw1/VPOz+veF4SXNTRWSMOP5XRFa525GaYaSstoV0j2sggklKEn5y8TzGjx7FdY+tor7lwPGIfR1dXP/4KgR44IqFQXfQOnFmPjeeMpPnVu3imRX7xyMa9nXw8vrdnDt/sud+1YsWTUMVnlu1/3XqWzu4/vFV5Gen8bNLghduO2/+FK44rpBf/WMbf//w4C6qX729jbc2V3LnOXMOmKbYe0OX/pREWMXVq96bOXmxsaIBEThy0sEzXeZNG0t6jMchlvlrmJKTEXG3ZrwlB1prEU4FDrcgcLjykiC+qKoNwOnAeOBqrPtn2AlUcR1MX/C4rDTuv2IBu+v3cduzaw9oBfy/v2xi/a4GfnLJ/LAfCDeeOpMTDs/jzhfXs8ndo+DPayto6+zmokXeZ34U5mVyXFEuz64sQ1UPKNx2/xULw+4+962z5zBn0hhueWptzw574PyB//jVzZwzbzKfO+7Aab2BjeW9XFX6K5tIT0mK+patvTdz8mJDeQNF+VkHLTgESE9JZvH0cTFbMNfdrSwrqfa0WU4iKvblsb26hYp676Vmlm6rZvSolIhm+yU6Lwki8IlyFvBbVV3b65gZJkprWzyV2OjPwsJx3HHWkby+cQ8Pv1MCwEtry/n9sh1c+wkfn+qniyg5Sbj30gWMyUjl+sdW0dTWybMrS5k5Ppt5ERY2u3jxNLZXt7BiRy2PHFC4LfyCtkBZ7a5u5YbHnSJtlY1t3PjEaqbnZfH9zx4TNJEu8eXx/vaafqcfBgaovZae9irSzZw2ljf0LJALZokvj00VDdQ2R38cYvOeRupaOoZtd8tAZo0t9VdzXFFuxGttEpmXBLFSRF7DSRCvishoYPjUXDaAt30gvPriCdP59FET+MHLH/LsyjLu+OM6Fh02jv/69BH9PxkoGO0Uadte3cy/P/o+q3bWcfHiqRG3bs46ZiJZacn84OUP+X6EhdsCRdrWlNbxvb9u4uanVlPf2sGDVy4MOb+82JdHS3sXH+wKX0gt2lNcD4zB22ZOdS3t7KprDTr+sP+1nA/BQGmIaAp0zxQP0xZEYNbYsm3eWljlda3sqG4ZUd1L4G0l9TU4M438qtoiInk43UzD1tJt1by5Ofwyi1GpyXz5JF/Q5nki6Ozq5pmVZZxx1MSw3SkAjfs6qGvpiFpfsIjww4vmcc7973DbM2sZl5nKA1csiGjlaLEvj6+efgQ/enUzyUnC+QsiWyEMzpz5z8ydxNMrypiWmxFx4bazjpnEF46fzu/+tR2AH144lyPDfKB62Vi+o6ubnTUtnNmr0GA09d7MKdzvM7DFaLjujrlTc8hITWaZv4Yzjp4U1TiX+qspzM2MejfbUElKEo6NYNZYoKUxXLvUQun3009Vu0WkBJglIt6XcCawjRUN/H5p6OmairKvo5vDcjO5cFHoRVvxdO/ftvDAm1v5aE8jd51zVNjHDnYGUzBjM1L5xZULueWpNdx59pwBbf7+nyfNwF/pbKozfvTA/mt94fgiPtjVwD0XHjOgwm3fOOtItlc7XUIXLw7/u/aysfzOmhY6uxVffnRnMAXMHO9s5rTUXx12SnC4GUwBaSlJLJ4+Lup1mbq7lfdKavj0UbFdpR1rS3x5vL5xD7vqWvtNdMv81YzNSOXIiSNn/AG8ldr4d5xaSVOBNUAxzkrqU2IbWuxcc2JR2K6I7m5l4XdfZ5m/OiETxFub9/LAm1tJT0nixTXl3HHmkWH3vY2kzHckjp4yltdvPWnAz09KEn5yybxBxTBn8hhevunjA35+WkoSv7v6WM+PX+LL4+kVZbR3dgd9z/dPcY1NF1NSkrOZ03J/TdgiihvLGxg/Op2C0eFnrRX78vjRq5upbmojb4Az3A46d0UD9a0dw/5qumccYlv/nwOB8YdojzvFm5c+gZuAjwE73HURC3CK7Y1YA1mUNFTK61q55ak1zJ44mnsvne/ushZ+T93ATJ1otiAOVYGrQ1bIAAAeHklEQVSN5UOtBSlxq7jGqgUB3jZz2ljR4Gk17/5xiOjNZhop0z1nT3RmjfX3OVBW20JpTeuw//cG4yVB7FPVfdBTpO9DwNto5DBW7MvzNBg4lDq6uvmKuzXig1cu5FNzJri7rIWvcVRW20pGajJ5Ee7Zaw52XD+zW/yVzeRlpUVtn4Jg+pths6+jiy17mzxNt5w7dSyZaclR7WZa5q9mel7mgLodE0lg1lh/M5kCU4WHe4spGC8JokxEcoAXgNdF5EWgPLZhxV/PoqQEakX86NXNTo2gC+cyoyCblOQkPrtwar+7rJXWtAx6DYRxBDaWD3VV6a+M3QymgP42c9qyp4mubmXOpP6nDacmJ/Gx6QMvTtdXV7eyvKRmxHxYepk1tsxfzbjMVI6YENvS2/HQb4JQ1QtUtU5V7wbuBB4Gzo91YPE2a/xocrPShnyD91Be37iHh97287niwgPKEF+8eGq/u6xFc4qrca7gV+6oDbqxvL+qKabdS3DwZk59BfaA8FowrtiXx5a9TVQ2DrSC/34byxto3Nc5Yrpbes8aC2XptmqOK8obceMPEL6aa27fG/AB8A4Q27+ABBAYhwj1RziUSmta+OrTazh6yhi++Zk5B/xsRkE2C91d1kLFWVbbMuzKHSSyUBvL17d2UNXUTlGMWxDgjENU1O9jR/XBV7YbKxrITk+h0OPvPHC1H431ECNl/CGg96yxYErd2lgjpcXUV7gWxEpghft1ZZ/7K2IfWvwtmeEMBgZmAcWDs7fCKhT4xRWLgtYqunjxNLbsbWJt2cELuOpbO2jY12ktiCgKbCzft98+UIMpmlVcQwl8AAf74NpQ3sCRk0Z7vqI9evIYZ5OcKIxDLPVX48vPYsKYETEjvmfW2LJtwS8Ul46whNhXuGquRarqc78W9bkfetPYEaTnjzCOG4h/76+bWFtWz48umkdhiE3fPzN3EqNSk4KWv94/g8laENES2Fi+b7eDv9KdwRTlKq7BzCjIomB0+kExdHcrmyoawq5/6CslOYmPTR/8/hCdXd28V1IzbFdPh1Lsy6O8fl/QWWPL/NXkZqUxa8LI7FQZGUXLY2RmP4OBsfbXDyr43b+288UTijjj6NArc3t2WVt78C5rsVgkZ9xxiJ21B7zf/spmkpPEc9fOYAT2qFja58p2R00LLe1dYWswBVPsy2NbZTN7B7BJTsCG8gaa2kbO+EPAkp7WWtUBx1WVZduqKfbljtgJIJYgwuhvMDCc51aV8cr63QM+9666Vr727DrmT8vh9jNn9/v4ixdPozHILmuB2RfTrAURVUtm5NHe2c2a0v3rIUqqmpk2LiPsosWoxuDLY29jW0/XFuwfoI60omigD/2drVX9PDK0wHOL3ZIkI0Vg1ljfCSs7a1oor983bAsSehFukNpb1bMRrjjMYGAoLe2d3PnCen751tYBn/cfmytpauvkRxfN9fSBE2qXtbLaVrLSksmJ4bz8Q9Gx0w8eh9hW2TQk3UsBgQ/13i3cjeUNpCQJMyPs8jhq8lh8+Vnc88qHVDVFPpvJX9nEL9/aRrEvd8BlUxJV4EKxb2ttpA3IBxPuk+dZABF5Y4hiSUhL+lmUFMxfP9hNc3sX/srmAc+A8lc2MSo1iRkeP3CSkoQLF009aJc1Z4pr5ohtAsfL2MxUjpo8pufDubtb2V7dPCQD1AHT8zKZMCb9gCvbDeUNHD4+O+iGTeEkJwkPXLGQ2pYObnlqDV3d3v/f7uvo4rrHVpGaLPz0kvkRnXe4WOLLY3fDgReKS7dVk5+dzuHjR+b4A4RPEEkichdOkb5b+96GKsB4CwwGRjIOEbiKb2zrpHIAV2MA/qpmpudFtqfAxYumHrTLmjPF1cYfYmGJL481O+vY19FFeX0r+zq6h7QFISIs6XNlu7GiYcAb1syZPIb/Ofco/rmligff9N76/fafNvDh7kZ+eul8Jg/T6q396TtrTFVZ5q8Z0eMPED5BXAbswynoNzrI7ZAQGAz0Og6xs7qF5SU1HFfk9MMGirdFyl/Z5Ln1EDAtN5Ni34G7rAVaECb6in15tHd1s2pHbcy2GfUSQ1VTG9sqm9jbuI/KxraIB6h7u/Rj07hgwRR+9rePeNfDeMRzq8p44r1Srjt5Bp88YvyAz5voei4U3S7F7dUt7G7YN6K7lyD8NNfNqnoPzpaj3+57G8IY467Yl8uehgMHA0N5dmUpIvRsnjOQBNHe2U1pbeuASjZctGj/Lmv1rR00tdkaiFj5WFEuSeJ0PwZ+zzOGYJFcb/vHIWo8lfjuj4jw3fOPZkZBNjc9uTrsrKYtexr57+fXc2xRLrd+ataAzzkcBFprgQvFkbr/Q19eplv8S0R+KiIr3NtPRGTglyjD0P5xiPBlN7q7lT+u2sWJh+ezsHAco1KTeqp7RmJnTQtd3TqgBBHYZe2ZFaU2xTXGxoxK5egpY1nqr8Zf2UR2ekq/5bWjrTA3k0ljR7FsW7WnTYK8yEpP4ZdXLqS5rYuvPLGaziBbrLa0d3LdY6vISk/m/ssXkBLBZlHDVbE7a8xf1czSbdUUjE4f0jGnePDyW30EaAQucW8NwG9jGVSiKcrPYsKY/schlvqr2VXXysWLp5GUJEzPyxpQCyKw4KpoADV9Arus/WVdBZt3NwK2SC6WlvjyWFNax8aKBorys4a8P7r3le2GXQ1MHZfB2IzBz1ibOWE03z3/aJaX1HDv37Yc8DNV5ZvPr2drZRP3XbZgxKya7k9Pa21bNcv81Szx5Y3o8QfwliBmqOpdqup3b98GDomV1AFexyGeWVHK6FEpnD7H2UnLV5CF30O3VF+B5wy0KujFi6fR3N7Fb/7pB2wNRCwV+/Lo6FLe314b8yqu4WKobm7nzc17B9W91NeFi6Zy6eJpPPDmVt7qtUXv0ytKeW71Lm46dSYnHJ4ftfMlusCssSfe28nexrYRP/4A3hJEq4icGLgjIicA8StOFCdLfHlUNraxLUSLoGFfBy+v38258yb31Evy5Wezs6aF9s6Dm+jhlFQ2k5+dzphRA7sSXHzYOIrys/hwdyOj01MYk5GY+2qPBB8ryiXZnWkW6yquoQSubAeygro/3z7vKGZPHM0tT62hvK6VTRUNfOvFDZx4eD5fOWVmVM+V6AKttQ3uWM9IH38Abwniy8CDIrJdRLYDDwBfimlUCShccTSAP6+toK2z+4B9gn0FWXR1K6W1kW065K9qGtTVqIhwkbtF4hTbByKmstNTOGaK86EcrxbEtNzMnj2TBzv+0Neo1GR+ceXCnqKR1z+2irEZqdx72fyexHgoCXwOTBwziukhaqONJF72g1irqvOAucBcVV2gqutiH1piOSzPHQwMkSCeWVnKzPHZzJu6/wouMCc+0nEIf+XgF1xdsGAKIliZ7yEQ+NAY6imuwWLwugdEJHwF2fzgwrms2lnH9upm7r98AflR2r96uAm0Gkb6+ocAz30PqtoQy0ASXaB5+faWyoM2i9+6t5HVO+v4xlmzDzge+MBwBp0neDpPfUsH1c3tg74anZyTwTfOPJJZEw+ZJStxc8Wxhezr6GJ2HN/rq0+YzoQx6UwaG5sB43PmTaaysY1xWak9264eigpzM7nhk4fzqTne/p6HO+ucjkCxL4/nVu9i694mZvbaXvCZlWUkJwnnL5hywOPHZqSSn50WUQvCH8VN7//jE4fUXIK4KczL5O5zj4prDEdPGcvRU2I7+/yLJ1p5NhHhNneN06Fg5E9ejqJgxdE6u7p5btUuPnlEQdAiZb78bE8L7AICySRe/dnGGBPQb4IQkUwRuVNEfuPenykiZ8c+tMQzdVwGU3IyDhiHeHtLJZWNbVy0aFrQ5zhTXb0vlvNXNZGSJDZ2YIyJOy8tiN8CbcAS934Z8N2YRZTA9q+HqKHbrXb57MoycrPSOGV28Do0RflZVDW1U9/a4ekc/spmCnMzST0EVqYaYxKb14VyPwQ6AFS1FRj5w/chFPtyqWluZ8veJmqb2/nbxr2cN39yyD0b9s9k8taKKKlqtu4lY0xC8DJI3S4iGYACiMgMnBbFIWn/cnun0mV7VzcXh+hegv1jCSVVzSwoHBf2tbu7lZKqZj4xqyBK0RpjzMB5SRB3Aa8A00TkMeAE4AuxDCqRTR2XybTcDJb6qymrbeWoyWPCLk4qzM0kOUk8zWTaVddKW2d3XOfTG2NMQL8JQlVfF5FVQDFO19JNqjrwjWtdIpIMrAB2qerZ7hanTwK5wCrg31S1fbDniYXiojxeXFtOe2c3d58zJ+xjU5OTKMzN9DRQ3VODyRKEMSYBeJnFtBA4DKgAyoFCEZkhIoNdQ3ETsKnX/XuAn6nqTKAWuGaQrx8zgQ3r05KTOG/+lH4f78v3VtW1xB2nGMpdyYwxJhQvg9S/AJYBDwG/AZbiXOl/JCKnD+SkIjIV+Azwv+59AU7B3QcbeBQ4fyCvPRQCZQ1OmzOecVlp/T7eV5BFSVVzz8ynUPxVzYwelUJ+dv+vaYwxseYlQWwHFqjqYlVdBCwA1gOnAT8c4HnvBb4GBMqc5gF1qtrp3i8Dgl6ai8i1gc2LKisrB3j6wZmck8H3LjiGr316tqfHF+Vn09bZTXl9+CK4gRpMh0KNF2NM4vOSIGar6obAHVXdiJMw/AM5obvIbq+qrux9OMhDg15uq+pDbrJaXFAQv9k+VxxXyHSPYwWBmUz9dTM5U1yte8kYkxi8jCNsFpFf4nQrAVyK072Ujrs2IkInAOeKyFnAKGAMTosiR0RS3FbEVJzxjhFhf4JoCjmFtbW9i111rTZAbYxJGF5aEF8AtgI3A7cAfvdYB/DJSE+oqneo6lRVnQ5cBvxdVa8E3gQuch92FfBipK+dqAqy0xmdnhK2JlPgZ0W2SM4YkyC8THNtBX7i3vryXmSof18HnhSR7wKrgYej+NpxJSIU9bP9aDSruBpjTDT0myBEZCbwfWAOTpcQAKo66FrSqvoW8Jb7vR84drCvmah8+Vm8v7025M9L3PEJWyRnjEkUXov1/RLoxOlS+j/g97EMaiTyFWSzq66V1vauoD/3VzUzJSeDjLTkIY7MGGOC85IgMlT1DUBUdYeq3o2zZsFEoHdNpmD8lU3WejDGJBQvCWKfiCQBW0TkBhG5AAhe29qEFPjwD5YgVNVZA2ED1MaYBOIlQdwMZAI3AouAzwGfj2VQI9GB+1MfqKqpnca2TpviaoxJKF4SxHRVbVLVMlW9WlUvBApjHdhIk5mWwuSxo4LOZPJbDSZjTALykiDu8HjM9MNXkB20BRFIGjYGYYxJJCGnuYrImcBZwBQR+XmvH43BmdFkIlSUn8ULa3ahqgfUW/JXNpGeksSUnIw4RmeMMQcKtw6iHGe/hnOB3nWTGnFWVJsI+QqyaNzXSVVTOwWj03uOl1Q1U5SfRVKSFekzxiSOkAlCVdcCa0XkcVUdSM0l00fv/al7Jwh/ZTOzJ42OV1jGGBOUlzGIY0XkdRH5SET8IlIiIgOq5HqoC8xS6j1Q3dHVzc6aFht/MMYkHC/VXB/G6VJaCQRfBmw8mZyTQVpK0gFrIXbWtNDZrVaDyRiTcLwkiHpVfTnmkRwCkpOEorysA2YyBWow2SI5Y0yi8ZIg3hSRHwHPAW2Bg6q6KmZRjWC+giw2727suW9VXI0xicpLgjjO/bq41zHF6jENiK8gi9c37qGjq5vU5CT8lc3kZaUxNjM13qEZY8wBvOwHEfGmQCa0ovxsOruV0poWd+Gc1WAyxiSmfmcxicgEEXlYRF52788RkWtiH9rI1Hd/an9Vs3UvGWMSkpdprr8DXgUmu/c/wingZwZghpsM/FVNNOzroKqpzVoQxpiE5CVB5Kvq00A3gKp2YtNdB2xsZip5WWn4K5t7WhG2BsIYk4i8JIhmEcnDGZhGRIqB+phGNcIV5Tv7U1sVV2NMIvMyi+lW4CVghoi8CxQAF8U0qhHOV5DF3z+spKSqmeQkoTA3M94hGWPMQbzMYlolIicBRwACbLbaTIPjK8jm6RVlrCmtozA3k7QULw05Y4wZWl5mMV0PZKvqBlVdD2SLyHWxD23kCow5LPfX2PiDMSZhebl0/Q9VrQvcUdVa4D9iF9LIN8OdtdTe1W3bjBpjEpaXBJEkvXa3EZFkIC12IY18hblZJLt7P9gAtTEmUXlJEK8BT4vIqSJyCvAE8EpswxrZ0lKSmDbO2T3O1kAYYxKVl1lMXwOuBf4TZ5D6NeB/YxnUoaAoP4vt1S3WxWSMSVhhE4TbnfSoqn4O+NXQhHRomDcthw93Nx6ws5wxxiSSsAlCVbtEpEBE0lS1faiCOhRcd/LhXH18Eb2Gd4wxJqF46WLaDrwrIi8BPVuhqepPYxXUoSAtJcnWPxhjEpqXBFHu3pKA0bENxxhjTKLwspL62wAikqWqzf093hhjzMjgZSX1EhHZCGxy788TkV/EPDJjjDFx5aUT/F7g00A1gKquBT4Ry6CMMcbEn6dRUlUt7XPI9oMwxpgRzkuCKBWR4wEVkTQRuQ23u2kgRGSaiLwpIptEZIOI3OQezxWR10Vki/t13EDPYYwxZvC8JIgvA9cDU4BdwHz3/kB1Al9V1SOBYuB6EZkD3A68oaozgTfc+8YYY+LEyyymKuDKaJ1QVSuACvf7RhHZhJN8zgNOdh/2KPAW8PVondcYY0xkvMxi8onIn0SkUkT2isiLIuKLxslFZDqwAFgOTHCTRyCJjA/xnGtFZIWIrKisrIxGGMYYY4Lw0sX0OPA0MAmYDDyDU9F1UEQkG/gjcLOqNnh9nqo+pKqLVXVxQUHBYMMwxhgTgpcEIar6e1XtdG9/AHQwJxWRVJzk8JiqPuce3iMik9yfTwL2DuYcxhhjBsdLgnhTRG4XkekicpiIfA34izvrKDfSE7qbDz0MbOpTz+kl4Cr3+6uAFyN9bWOMMdEjquEbAyJSEubHqqoRjUeIyInAP4EPgG738DdwxiGeBgqBncDFqloT7rUWL16sK1asiOT0xhhzyBORlaq6uL/HeZnFVBSdkHpe7x2cjYeCOTWa5zLGGDNwVm/aGGNMUJYgjDHGBGUJwhhjTFBeNgxCRKYAh/V+vKq+HaugjDHGxF+/CUJE7gEuBTayv4qrApYgjDFmBPPSgjgfOEJV22IdjDHGmMThZQzCD6TGOhBjjDGJxUsLogVYIyJvAD2tCFW9MWZRGWOMiTsvCeIl92aMMeYQ4mUl9aNDEYgxxpjEEjJBiMjTqnqJiHxAkOqtqjo3ppEZY4yJq3AtiJvcr2cPRSDGGGMSS7gEcamIvAusVtXOoQrIGGNMYgiXIKYC9wGzRWQd8C/gXWBpf2W4jTHGDH8hE4Sq3gYgImnAYuB44IvAb0SkTlXnDE2Ixhhj4sHLNNcMYAww1r2V42z2Y4wxZgQLN4vpIeAooBFnt7d/AT9V1dohis0YY0wchSu1UQikA7uBXUAZUDcUQRljjIm/cGMQZ4iI4LQijge+ChwtIjU4A9V3DVGMxhhj4iDsGISqKrBeROqAevd2NnAsYAnCGGNGsHBjEDfitBxOADpwp7gCj2CD1MYYM+KFa0FMB54FblHViqEJxxhjTKIINwZx61AGYowxJrF42TDIGGPMIcgShDHGmKAsQRhjjAnKEoQxxpigLEEYY4wJyhKEMcaYoCxBGGOMCcoShDHGmKAsQRhjjAnKEoQxxpigLEEYY4wJyhKEMcaYoBIqQYjIGSKyWUS2isjt8Y7HGGMOZQmTIEQkGXgQOBOYA1wuInPiG5Uxxhy6EiZB4OxSt1VV/araDjwJnBfnmIwx5pAVdsvRITYFKO11vww4ru+DRORa4Fr3bpOIbA7xevlAVVQjjD6LMTqGQ4wwPOK0GKMj0WM8zMuDEilBSJBjetAB1YeAh/p9MZEVqro4GoHFisUYHcMhRhgecVqM0TEcYvQikbqYyoBpve5PBcrjFIsxxhzyEilBvA/MFJEiEUkDLgNeinNMxhhzyEqYLiZV7RSRG4BXgWTgEVXdMIiX7LcbKgFYjNExHGKE4RGnxRgdwyHGfonqQd38xhhjTEJ1MRljjEkgliCMMcYENSITRCKU7BCRaSLypohsEpENInKTe/xuEdklImvc21m9nnOHG/NmEfn0EMa6XUQ+cONZ4R7LFZHXRWSL+3Wce1xE5OdunOtEZOEQxHdEr/drjYg0iMjN8X4vReQREdkrIut7HYv4fRORq9zHbxGRq4Ygxh+JyIduHM+LSI57fLqItPZ6P3/V6zmL3P8jW91/R7Bp6dGMMeLfbSz/7kPE+FSv+LaLyBr3eFzex5hQ1RF1wxng3gb4gDRgLTAnDnFMAha6348GPsIpIXI3cFuQx89xY00Hitx/Q/IQxbodyO9z7IfA7e73twP3uN+fBbyMs26lGFgeh9/vbpyFPnF9L4FPAAuB9QN934BcwO9+Hed+Py7GMZ4OpLjf39Mrxum9H9fndd4DlrjxvwycGeMYI/rdxvrvPliMfX7+E+Bb8XwfY3EbiS2IhCjZoaoVqrrK/b4R2ISzWjyU84AnVbVNVUuArTj/lng5D3jU/f5R4Pxex/9PHcuAHBGZNIRxnQpsU9UdYR4zJO+lqr4N1AQ5dyTv26eB11W1RlVrgdeBM2IZo6q+pqqd7t1lOGuOQnLjHKOqS9X5lPu/Xv+umMQYRqjfbUz/7sPF6LYCLgGeCPcasX4fY2EkJohgJTvCfTDHnIhMBxYAy91DN7jN+0cCXRDEN24FXhORleKUMgGYoKoV4CQ7YHwCxAnO+pjef4iJ9l5G+r7F+/38Is6VbECRiKwWkX+IyMfdY1PcuAKGKsZIfrfxfB8/DuxR1S29jiXS+zhgIzFBeCrZMVREJBv4I3CzqjYAvwRmAPOBCpymKcQ37hNUdSFOJd3rReQTYR4btzjFWUB5LvCMeygR38tQQsUUz/fzv4FO4DH3UAVQqKoLgFuBx0VkTJxijPR3G8/f+eUceNGSSO/joIzEBJEwJTtEJBUnOTymqs8BqOoeVe1S1W7gN+zv+ohb3Kpa7n7dCzzvxrQn0HXkft0b7zhxEtgqVd3jxptw7yWRv29xidUdDD8buNLt7sDttql2v1+J06c/y42xdzdUzGMcwO82Xu9jCvBZ4KnAsUR6HwdrJCaIhCjZ4fZLPgxsUtWf9jreu7/+AiAwK+Il4DIRSReRImAmzoBWrOPMEpHRge9xBjDXu/EEZtRcBbzYK87Pu7NyioH6QJfKEDjgSi3R3ste547kfXsVOF1ExrndKKe7x2JGRM4Avg6cq6otvY4XiLMvCyLiw3nf/G6cjSJS7P6//nyvf1esYoz0dxuvv/vTgA9VtafrKJHex0GL9yh5LG44M0Y+wsnc/x2nGE7EaT6uA9a4t7OA3wMfuMdfAib1es5/uzFvZohmN+DM+ljr3jYE3i8gD3gD2OJ+zXWPC87GTtvcf8fiIYozE6gGxvY6Ftf3EidZVQAdOFeH1wzkfcMZB9jq3q4eghi34vTXB/5f/sp97IXu/4G1wCrgnF6vsxjnQ3ob8ABuFYYYxhjx7zaWf/fBYnSP/w74cp/HxuV9jMXNSm0YY4wJaiR2MRljjIkCSxDGGGOCsgRhjDEmKEsQxhhjgrIEYYwxJihLEMb0IiJdcmDl2LBVQUXkyyLy+Sicd7uI5A/2dYyJJpvmakwvItKkqtlxOO92nLURVUN9bmNCsRaEMR64V/j3iMh77u1w9/jdInKb+/2NIrLRLTD3pHssV0RecI8tE5G57vE8EXnNLej2a3rV6RGRz7nnWCMivw6syjVmqFmCMOZAGX26mC7t9bMGVT0WZwXsvUGeezuwQFXnAl92j30bWO0e+wZOiWeAu4B31Cno9hJQCCAiRwKX4hRQnA90AVdG959ojDcp8Q7AmATT6n4wB/NEr68/C/LzdcBjIvIC8IJ77ESc0guo6t/dlsNYnA1oPuse/4uI1LqPPxVYBLzvlOshg/0F/4wZUpYgjPFOQ3wf8BmcD/5zgTtF5CjCl3gO9hoCPKqqdwwmUGOiwbqYjPHu0l5fl/b+gYgkAdNU9U3ga0AOkA28jdtFJCInA1Xq7AvS+/iZONuNglPg7yIRGe/+LFdEDovhv8mYkKwFYcyBMsTdfN71iqoGprqmi8hynAury/s8Lxn4g9t9JMDPVLVORO4Gfisi64AW9pcC/zbwhIisAv4B7ARQ1Y0i8k2cHf6ScKqHXg+E22LVmJiwaa7GeGDTUM2hyLqYjDHGBGUtCGOMMUFZC8IYY0xQliCMMcYEZQnCGGNMUJYgjDHGBGUJwhhjTFD/H7ykGJMZu1qmAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-d6c5abf20189>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mAsyncSelfPlay\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_envs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_each\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_games\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-19-82ecbd35ddfc>\u001b[0m in \u001b[0;36mAsyncSelfPlay\u001b[0;34m(player, n_envs, n_games, test_each, test_games)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m                 \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_boards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m                 \u001b[0mold_board\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_board\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menvs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpossible_moves\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mplayer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-57ec72e2a717>\u001b[0m in \u001b[0;36msample_action\u001b[0;34m(self, states)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m         \u001b[0mprobs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_s\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_actor_policy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_afterstates\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstates\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    885\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    886\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 887\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    888\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    889\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1109\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1110\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1111\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1112\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1284\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m       return self._do_call(_run_fn, feeds, fetches, targets, options,\n\u001b[0;32m-> 1286\u001b[0;31m                            run_metadata)\n\u001b[0m\u001b[1;32m   1287\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1288\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1290\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1291\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1292\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1293\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1275\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extend_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1276\u001b[0m       return self._call_tf_sessionrun(\n\u001b[0;32m-> 1277\u001b[0;31m           options, feed_dict, fetch_list, target_list, run_metadata)\n\u001b[0m\u001b[1;32m   1278\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1279\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.6/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_call_tf_sessionrun\u001b[0;34m(self, options, feed_dict, fetch_list, target_list, run_metadata)\u001b[0m\n\u001b[1;32m   1365\u001b[0m     return tf_session.TF_SessionRun_wrapper(\n\u001b[1;32m   1366\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1367\u001b[0;31m         run_metadata)\n\u001b[0m\u001b[1;32m   1368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1369\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_tf_sessionprun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "AsyncSelfPlay(player = AC, n_games = 10000, n_envs = 100, test_each = 50, test_games = 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
