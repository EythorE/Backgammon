{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import Backgammon as B\n",
    "import agent as A\n",
    "import flipped_agent as FA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "import keras.layers as L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "states = tf.placeholder(\"float32\", (None, 29), name = \"states\")\n",
    "actions = tf.placeholder(\"int32\", name = \"action_ids\")\n",
    "cumulative_rewards = tf.placeholder(\"float32\", name = \"cumulative_rewards\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(L.Dense(32, activation = \"relu\"))\n",
    "model.add(L.Dense(64, activation = \"relu\"))\n",
    "model.add(L.Dense(1))\n",
    "\n",
    "logits = model(states)\n",
    "policy = tf.nn.softmax(logits)\n",
    "log_policy = tf.nn.log_softmax(logits)\n",
    "\n",
    "get_action_prob = lambda s: policy.eval({states: s})\n",
    "\n",
    "indices = tf.stack([tf.range(tf.shape(log_policy)[0]),actions],axis=-1)\n",
    "log_policy_for_actions = tf.gather_nd(log_policy,indices)\n",
    "J = tf.reduce_mean(log_policy_for_actions * cumulative_rewards)\n",
    "entropy = -tf.reduce_sum(tf.multiply(policy, log_policy), 1, name=\"entropy\")\n",
    "all_weights = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES)\n",
    "\n",
    "loss = -J -0.1 * entropy\n",
    "\n",
    "update = tf.train.AdamOptimizer().minimize(loss,var_list=all_weights)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[6 1]\n"
     ]
    }
   ],
   "source": [
    "board = B.init_board()\n",
    "dice = B.roll_dice()\n",
    "legal_moves, legal_boards = B.legal_moves(board, dice, 1)\n",
    "print(dice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 29)"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([board for board in legal_boards]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28, 2, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([move for move in legal_moves]).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[13,  6],\n",
       "       [ 7,  5]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "legal_moves[3].T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0. -2.  1.  0.  0.  0.  5.  0.  2.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  1.  0.  1.  4.  0.  2.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  1.  0.  0.  5.  1.  1.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  1.  0.  0.  5.  0.  2.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  0.  0.  1.  4.  0.  4.  0.  0.  0. -5.  4.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  0.  0.  0.  5.  1.  3.  0.  0.  0. -5.  4.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  0.  0.  0.  5.  0.  4.  0.  0.  0. -5.  4.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  1.  0.  1.  4.  0.  2.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  0.  0.  1.  4.  0.  4.  0.  0.  0. -5.  4.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  1.  0.  0.  0.  5.  0.  2.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  1.  0.  0.  5.  1.  1.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  0.  0.  0.  5.  1.  3.  0.  0.  0. -5.  4.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  0.  2.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  1.  0.  0.  5.  0.  2.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  0.  0.  0.  5.  0.  4.  0.  0.  0. -5.  4.  0.  0.  0. -3.\n",
      "   0. -5.  0.  0.  0.  1.  1.  0.  0.  0.  0.]\n",
      " [ 0. -2.  0.  0.  0.  0.  5.  0.  3.  0.  0.  0. -5.  5.  0.  0.  0. -3.\n",
      "   1. -5.  0.  0.  0.  0.  1.  0.  0.  0.  0.]] [array([[8, 3],\n",
      "       [3, 2]]), array([[8, 3],\n",
      "       [6, 5]]), array([[8, 3],\n",
      "       [8, 7]]), array([[ 8,  3],\n",
      "       [24, 23]]), array([[13,  8],\n",
      "       [ 6,  5]]), array([[13,  8],\n",
      "       [ 8,  7]]), array([[13,  8],\n",
      "       [24, 23]]), array([[6, 5],\n",
      "       [8, 3]]), array([[ 6,  5],\n",
      "       [13,  8]]), array([[8, 7],\n",
      "       [7, 2]]), array([[8, 7],\n",
      "       [8, 3]]), array([[ 8,  7],\n",
      "       [13,  8]]), array([[24, 23],\n",
      "       [ 8,  3]]), array([[24, 23],\n",
      "       [13,  8]]), array([[24, 23],\n",
      "       [23, 18]])]\n"
     ]
    }
   ],
   "source": [
    "s = tf.InteractiveSession()\n",
    "s.run(tf.global_variables_initializer())\n",
    "board = B.init_board()\n",
    "legal_moves, legal_boards = B.legal_moves(board, B.roll_dice(), 1)\n",
    "\n",
    "legal_boards = np.array([board for board in legal_boards])\n",
    "\n",
    "probs = get_action_prob(legal_boards)\n",
    "n_actions = probs.shape[0]\n",
    "\n",
    "probs = probs.reshape(n_actions)\n",
    "probs = probs / np.sum(probs)\n",
    "action = np.random.choice(np.arange(0, n_actions),\n",
    "                         p = probs)\n",
    "\n",
    "move = legal_moves[action]\n",
    "\n",
    "print(legal_boards, legal_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_session():\n",
    "    \"\"\"play env with REINFORCE agent and train at the session end\"\"\"\n",
    "    \n",
    "    #arrays to record session\n",
    "    boards, rewards = [],[]\n",
    "    \n",
    "    board = B.init_board()\n",
    "    player = 1\n",
    "    \n",
    "    while True:\n",
    "        \n",
    "        dice = B.roll_dice()\n",
    "        \n",
    "        for i in range(1 + int(dice[0] == dice[1])):\n",
    "        \n",
    "            legal_moves, legal_boards = B.legal_moves(board, dice, 1)\n",
    "            legal_boards = np.array([board for board in legal_boards])\n",
    "            \n",
    "            if legal_boards.shape[0] == 0:\n",
    "                break\n",
    "            \n",
    "            probs = get_action_prob(legal_boards)\n",
    "            n_actions = probs.shape[0]\n",
    "            probs = probs.reshape(n_actions)\n",
    "            probs = probs / np.sum(probs)\n",
    "\n",
    "            action = np.random.choice(np.arange(0, n_actions), \n",
    "                                 p = probs)\n",
    "\n",
    "            move = legal_moves[action]\n",
    "\n",
    "            next_board = B.update_board(board = board, move = move.T, player = player)\n",
    "\n",
    "            #record session history to train later\n",
    "            boards.append(next_board)\n",
    "\n",
    "            board = next_board\n",
    "            board = FA.flip_board(board)\n",
    "\n",
    "            if B.game_over(board):\n",
    "                rewards.append(1)\n",
    "            else:\n",
    "                rewards.append(0)\n",
    "                \n",
    "            if B.check_for_error(board):\n",
    "                return(\"Error\")\n",
    "            \n",
    "    train_step(boards, rewards)\n",
    "            \n",
    "    return sum(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_cumulative_rewards(rewards, #rewards at each step\n",
    "                           gamma = 0.99 #discount for reward\n",
    "                           ):\n",
    "    \"\"\"\n",
    "    take a list of immediate rewards r(s,a) for the whole session \n",
    "    compute cumulative rewards R(s,a) (a.k.a. G(s,a) in Sutton '16)\n",
    "    R_t = r_t + gamma*r_{t+1} + gamma^2*r_{t+2} + ...\n",
    "    \n",
    "    The simple way to compute cumulative rewards is to iterate from last to first time tick\n",
    "    and compute R_t = r_t + gamma*R_{t+1} recurrently\n",
    "    \n",
    "    You must return an array/list of cumulative rewards with as many elements as in the initial rewards.\n",
    "    \"\"\"\n",
    "    \n",
    "    rewards = np.array(rewards)\n",
    "    R = np.zeros_like(rewards, dtype= \"float32\")\n",
    "    r = 0.\n",
    "    for i, reward in enumerate(reversed(rewards)):\n",
    "        r += reward\n",
    "        R[-(i + 1)] = r\n",
    "        r *= gamma\n",
    "        \n",
    "    return R\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(_states,_actions,_rewards):\n",
    "    \"\"\"given full session, trains agent with policy gradient\"\"\"\n",
    "    _cumulative_rewards = get_cumulative_rewards(_rewards)\n",
    "    update.run({states:_states,actions:_actions,cumulative_rewards:_cumulative_rewards})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Too many or too few pieces on board!\n",
      "Error at game step  6\n"
     ]
    }
   ],
   "source": [
    "boards, moves, rewards = [], [], []\n",
    "    \n",
    "board = B.init_board()\n",
    "player = 1\n",
    "\n",
    "k = 1\n",
    "error = False\n",
    "while True:\n",
    "    dice = B.roll_dice()\n",
    "\n",
    "    for i in range(1 + int(dice[0] == dice[1])):\n",
    "\n",
    "        legal_moves, legal_boards = B.legal_moves(board, dice, 1)\n",
    "        legal_boards = np.array([board for board in legal_boards])\n",
    "\n",
    "        if len(legal_moves) == 0:\n",
    "            break\n",
    "\n",
    "        probs = get_action_prob(legal_boards)\n",
    "        n_actions = probs.shape[0]\n",
    "        probs = probs.reshape(n_actions)\n",
    "        probs = probs / np.sum(probs)\n",
    "\n",
    "        action = np.random.choice(np.arange(0, n_actions), \n",
    "                             p = probs)\n",
    "\n",
    "        move = legal_moves[action]\n",
    "\n",
    "        next_board = B.update_board(board = board, move = move.T, player = 1)\n",
    "\n",
    "        #record session history to train later\n",
    "        boards.append(next_board)\n",
    "        moves.append(move)\n",
    "        board = next_board\n",
    "\n",
    "        if B.game_over(board):\n",
    "            rewards.append(1)\n",
    "            break\n",
    "        else:\n",
    "            rewards.append(0)\n",
    "            \n",
    "    board = FA.flip_board(board)\n",
    "    k += 1\n",
    "    if B.check_for_error(board):\n",
    "        Error = True\n",
    "        print(\"Error at game step \", k)\n",
    "        break\n",
    "        \n",
    "if not Error:        \n",
    "    train_step(boards, rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7, 29)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(boards).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(15.0, -15.0),\n",
       " (15.0, -15.0),\n",
       " (15.0, -15.0),\n",
       " (15.0, -15.0),\n",
       " (15.0, -15.0),\n",
       " (15.0, -15.0),\n",
       " (14.0, -15.0)]"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[(sum(board[board>0]), sum(board[board<0])) for board in boards]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 0., -2.,  0.,  0.,  0.,  1.,  5.,  0.,  4.,  0.,  0.,  0., -5.,\n",
       "         5.,  0.,  0.,  0., -3., -1., -5.,  0.,  0.,  0.,  0.,  1.,  0.,\n",
       "         0.,  0.,  0.])]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "boards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0., -2.,  1.,  0.,  1.,  0.,  4.,  0.,  3.,  0.,  0.,  0., -5.,\n",
       "        5.,  0.,  0.,  0., -3.,  0., -5.,  0.,  0.,  0.,  0.,  2.,  0.,\n",
       "        0.,  0.,  0.])"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FA.flip_board(board)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0., -2., -0., -0., -0., -0.,  5., -0.,  3., -0., -0., -0., -5.,\n",
       "        5., -0., -0., -0., -3., -0., -4., -0., -1., -0., -1.,  2., -0.,\n",
       "       -0., -0., -0.])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "board"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(legal_moves)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.94148016, 0.95099   , 0.960596  , 0.970299  , 0.9801    ,\n",
       "       0.99      , 1.        ], dtype=float32)"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_cumulative_rewards(rewards)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
